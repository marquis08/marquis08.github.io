---
date: 2021-08-02 14:28
title: "NLP - Transformer and BERT"
categories: DevCourse2 NLP
tags: DevCourse2 NLP
# ëª©ì°¨
toc: true  
toc_sticky: true 
toc_label : "Contents"
---

# ML ëª¨ë¸ ì´í•´ë¥¼ ìœ„í•œ íŒ
- ë³µì¡í•œ ML ëª¨ë¸ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¥¼ ë”°ë¼ê°ˆ ê²ƒ
  - ì´ ëª¨ë¸ì´ í’€ë ¤ê³  í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€
  - ì¶”ë¡  ë‹¨ê³„ë¥¼ ë¨¼ì € ì´í•´. "í•™ìŠµ"ëœ ëª¨ë¸ì´ ì–´ë–¤ "ì…ë ¥"ì„ ë°›ì•„ì–´ ì–´ë–¤ "ì¶œë ¥"ì„ ê³„ì‚°í•˜ëŠ”ì§€ íŒŒì•…í•  ê²ƒ
  - ë¬´ì—‡ì´ í•™ìŠµë˜ëŠ”ê°€(ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì—‡ì¸ê°€), ëª¨ë¸ íŒŒë¼ë¯¸í„°ì™€ ì…ë ¥ ê·¸ë¦¬ê³  ê·¸ ë‘˜ì˜ í•¨ìˆ˜ë¥¼ êµ¬ë¶„í•  ê²ƒ.
  - ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ê°€(ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ê°€), ì–´ë–¤ ì—ëŸ¬í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°€

# NLP ì™€ ë”¥ëª¨ë¸
- NLPë¥¼ ìœ„í•œ ë”¥ëª¨ë¸ë“¤
  - RNN(LSTM ë“±)
  - TextCNN
  - Transformer
  - BERT (Transformer encoder)
  - GPT (Transformer decoder)

## RNN
![encoder_decoder2.png](\assets\images\encoder_decoder2.png){: .align-center}
- ë¬¸ì œì 
  - ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë“¤ ê°„ì˜ ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ì–´ë ¤ì›€
  - ìˆœì°¨ì  ì†ì„±ìœ¼ë¡œ ì¸í•œ ëŠë¦° ì†ë„

## TextCNN
![textcnn-arch.png](\assets\images\textcnn-arch.png){: .align-center}
- ì†ë„ëŠ” ë¹ ë¦„(conv ê³„ì‚°ì˜ ë³‘ë ¬í™”)
- ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë“¤ê°„ì˜ ì˜ì¡´ì„± ì–´ë ¤ì›€
- ë‚˜ì˜ì§€ ì•Šì€ ì„±ëŠ¥

## Transformer ëª¨ë¸
- Attention Is All You Need
  - No RNN, No convolution
  - ì˜¤ì§ attentionìœ¼ë¡œë§Œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ì˜ í‘œí˜„í•  ìˆ˜ ìˆë‹¤
  - ë³‘ë ¬í™” ê°€ëŠ¥
  - BERT, GPT ë“±ì˜ ëª¨ë¸ì˜ ê¸°ë°˜
### ì´ ëª¨ë¸ì´ í’€ë ¤ê³  í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€
  - seq2seq
  - ìˆœì°¨ì ì…ë ¥ì— ëŒ€í•´ ìˆœì°¨ì  ì¶œë ¥ì„ ë°˜í™˜
  - ê¸°ê³„ë²ˆì—­
  - ì§ˆì˜ì‘ë‹µ

### ì¶”ë¡  ë‹¨ê³„ ì´í•´
![the_transformer_3.png](\assets\images\the_transformer_3.png){: .align-center}
![The_transformer_encoders_decoders.png](\assets\images\The_transformer_encoders_decoders.png){: .align-center}
- The encoding component is a stack of encoders (the paper stacks six of them on top of each other â€“ thereâ€™s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.
![The_transformer_encoder_decoder_stack.png](\assets\images\The_transformer_encoder_decoder_stack.png){: .align-center}
- The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:
![encoder_with_tensors.png](\assets\images\encoder_with_tensors.png){: .align-center}
- í•˜ë‚˜ì˜ ë‹¨ì–´ë“¤ì´ ê°ê° ë‹¨ì–´ ì„ë² ë”©ì„ ê°€ì§€ê³  ìˆìŒ($$x_1$$),
- ì´ ì„ë² ë”©ì´ self-attention ëª¨ë“ˆì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°
- self-attention ëª¨ë“ˆì„ í†µê³¼í•˜ë©´ì„œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ì‚¬ìš©í•´ì„œ ê·¸ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ëª…í™•í•˜ê²Œ í•¨.(self-attentionì€ í•´ë‹¹ ë‹¨ì–´ ì£¼ë³€ì˜ ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•´ ë¬¸ë§¥ì„ í™œìš©í•¨, ë¬¸ë§¥í™”ëœ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…)
- ìƒˆë¡œìš´ embeddingì´ self-attention ëª¨ë“ˆì„ í†µê³¼í•˜ë©´ì„œ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë¨(ì´ ê²°ê³¼ëŠ” ì£¼ë³€ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ë‹¤ ì‚¬ìš©í•œ ê²ƒ)
- ì´ ì¶œë ¥ ê°’(embedding)ì´ Feed Forwardë¥¼ ì§€ë‚˜ê°, ê°ê°ì˜ ë‹¨ì–´ë“¤ì´ ë³„ê°œë¡œ ì§€ë‚˜ê°(ë™ì¼í•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©, ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì˜ì¡´ì„±ì´ ì—†ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™” ê°€ëŠ¥, íˆìë§Œ self-attentionì€ ë‹¨ì–´ê°„ì˜ ê´€ê³„ë¥¼ ë´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”ëŠ” í˜ë“¬)
- self-attentionì„ í†µê³¼í•œ í›„ Feed Forwardë¥¼ ì§€ë‚˜ê°€ê²Œ í•˜ëŠ” ì´ìœ ëŠ” í‘œí˜„ë ¥ì„ ë†’ì´ê¸° ìœ„í•¨ì´ë¼ê³  í•¨.(êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ê²ƒ?)

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Self-Attention
![transformer_self-attention_visualization.png](\assets\images\transformer_self-attention_visualization.png){: .align-center}
- "The animal didn't cross the street because it was too tired"
- ì—¬ê¸°ì„œ "it"ì´ ê°€ë¦¬í‚¤ëŠ” ë‹¨ì–´ëŠ”?
- ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ë¬¸ë§¥ì— ì˜í•´ ê²°ì •ë¨. ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ë§¥ì— ì˜í•´ ëœ»ì´ ë‹¬ë¼ì§
- í˜„ì¬ ë‹¨ì–´ì˜ ì˜ë¯¸(ì„ë² ë”©ì„ í†µí•´ í‘œí˜„ë˜ëŠ”)ë¥¼ ì£¼ë³€ ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì˜ ì¡°í•©(weighted sum)ìœ¼ë¡œ í‘œí˜„
- As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
- ![transformer_self-attention-weighted-sum.png](\assets\images\transformer_self-attention-weighted-sum.png){: .align-center}
  - ì´ë ‡ê²Œ weighted sumì„ í™œìš©í•´ì„œ "it"ì˜ embeddingì„ ê³„ì‚°í•˜ê³  ë‚˜ë©´, ê´€ë ¨ìˆëŠ” ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© ì„±ë¶„ì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆê²Œ ë¨.
  - ì´ëŸ°ì‹ìœ¼ë¡œ í•˜ë©´ ë¬¸ë§¥ì ì˜ë¯¸ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ.
  - RNNê³¼ ë¹„êµí•˜ë©´, RNNì˜ ê²½ìš° ë¬¸ë§¥ì˜ ì •ë³´ê°€ hidden stateë¥¼ í†µí•´ ì „ë‹¬ë˜ëŠ”ë°, ì´ê²ƒì€ ì´ì „ ë‹¨ê³„ì˜ ì •ë³´ë§Œì„ ë‹´ê³  ìˆìŒ.(unidirectional, ë‹¨ë°©í–¥)
  - TransformerëŠ” bidirectional, ì¦‰ ë¬¸ë§¥ ì „ì²´ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í›¨ì”¬ ì˜ë¯¸ê°€ í’ë¶€í•´ì§

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Self-Attention - ë‹¨ì–´ ì„ë² ë”© ë²¡í„° ì¤‘ì‹¬ìœ¼ë¡œ
1. The first step in calculating self-attention is to create three vectors from each of the encoderâ€™s input vectors (in this case, the embedding of each word).
    ![transformer_self_attention_vectors.png](\assets\images\transformer_self_attention_vectors.png){: .align-center}
    - Multiplying $$x_1$$ by the $$WQ$$ weight matrix produces $$q_1$$, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.
    - $$q^T = x^{T}W^{Q}$$, $$k^{T} = x^{T}W^{k}$$, $$v^{T} = x^{T}W^{v}$$
    - ì—¬ê¸°ì—ì„œ ì…ë ¥ì€ $$x^T$$ì´ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” í–‰ë ¬ë“¤($$W^{q}, W^{k}, W^{v}$$) ì„.
    - $$\underbrace{q^{T}}_{\text{í•¨ìˆ˜ê°’}} = \underbrace{x^{T}}_{\text{ì…ë ¥}}\underbrace{W^{q}}_{\text{ëª¨ë¸ íŒŒë¼ë¯¸í„°}}$$
    - ë³´í†µ xì˜ ì°¨ì›ì€ 512, qì˜ ì°¨ì›ì€ 64ë¥¼ ë§ì´ ì‚¬ìš©í•¨.
2. Self-attention score: The second step in calculating self-attention is to calculate a score.
    ![transformer_self_attention_score.png](\assets\images\transformer_self_attention_score.png){: .align-center}
    - "Thinking"ì˜ scoreë¥¼ êµ¬í•  ë•ŒëŠ”, thinkingì˜ ì¿¼ë¦¬ë²¡í„°($$q_1$$)ì„ ê°€ì§€ê³  ë‚˜ë¨¸ì§€ ëª¨ë“  ë‹¨ì–´ë“¤ì˜ í‚¤ë²¡í„°($$k_1, k_2$$)ì™€ì˜ dot productë¥¼ êµ¬í•¨(ìê¸°ìì‹  í¬í•¨)
3. Divide the scores by 8 (the square root of the dimension of the key vectors used in the paper â€“ 64. This leads to having more stable gradients.
4. Pass the result through a softmax operation. Softmax normalizes the scores so theyâ€™re all positive and add up to 1.
    - ![self-attention_softmax.png](\assets\images\self-attention_softmax.png){: .align-center}
    - ë…¼ë¬¸ ìƒì—ëŠ” embeddingì˜ ì‚¬ì´ì¦ˆ 64ì˜ rootê°’ì¸ 8ë¡œ ë‚˜ëˆ ì¤Œ(gradient ê³„ì‚°ì„ ì•ˆì •ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´ì„œ)

    - softmax ë¡œ ë³€í™˜(ê°ê°ì˜ ê°’ì´ í™•ë¥ ë¡œ ë¨)
    - ![attention-matrix-form.png](\assets\images\attention-matrix-form.png){: .align-center .img-40}
5. The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). 
6. The sixth step is to sum up the weighted value vectors.
    - ![self-attention-output.png](\assets\images\self-attention-output.png){: .align-center}
    - not embedding vector, its value vector.(weighted)
    - ìì‹ ì˜ ê°€ì¤‘ì¹˜ì¸ value vectoì™€ softmaxë¥¼ ê³±í•œë‹¤.
      - $$z_1 = \text{softmax value}\times v_1 + \text{softmax value}\times v_2 = 0.88 \times v_1 + 0.12 \times v_2$$
    - ![attention-matrix-score.png](\assets\images\attention-matrix-score.png){: .align-center .img-40}


#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Self-Attention - í–‰ë ¬ì—°ì‚°ìœ¼ë¡œ í‘œí˜„
- x: ì…ë ¥
- $$W^{q}, W^{k}, W^{v}$$: ëª¨ë¸ íŒŒë¼ë¯¸í„°
- $$Q, K, V$$: ì…ë ¥ê³¼ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ê´€í•œ í•¨ìˆ˜ì˜ ì¶œë ¥ê°’
- The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices weâ€™ve trained (WQ, WK, WV).
  - ![self-attention-matrix-calculation.png](\assets\images\self-attention-matrix-calculation.png){: .align-center}
- Finally, since weâ€™re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.
  - í–‰ë ¬ì˜ dimension ì •ë¦¬
    - s: sequence length, h:size_per_head(íŒŒë¼ë¯¸í„°ì˜ ì—´ì˜ ê°œìˆ˜, 64), d: input embedding size(xì˜ ì—´ì˜ ê°œìˆ˜, 512)
    - ![attention-matrix-all.png](\assets\images\attention-matrix-all.png){: .align-center}
  - ![self-attention-matrix-calculation-2.png](\assets\images\self-attention-matrix-calculation-2.png){: .align-center}
    - $$z = s \times h$$, h: size_per_head

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Multi-headed attention
- ë‹¤ì–‘í•œ attention matrixë“¤ì„ ë°˜ì˜í•˜ê¸° ìœ„í•œ ë°©ë²•. ë¸”ë¡œê·¸ ì˜¤ì—­ ì£¼ì˜ğŸš¨
- cnn ì—ì„œ í•„í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬ í˜•íƒœë¥¼ ë³´ê¸° ìœ„í–ˆë˜ ê²ƒ ì²˜ëŸ¼ í•™ìŠµëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´.
![transformer_attention_heads_qkv.png](\assets\images\transformer_attention_heads_qkv.png){: .align-center}
![transformer_attention_heads_z.png](\assets\images\transformer_attention_heads_z.png){: .align-center}
- This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices â€“ itâ€™s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix. How do we do that? We concat the matrices then multiple them by an additional weights matrix $$W^O$$.
  - ![transformer_attention_heads_weight_matrix_o.png](\assets\images\transformer_attention_heads_weight_matrix_o.png){: .align-center}
  - í–‰ì˜ ê°œìˆ˜ëŠ” ìœ ì§€í•˜ë˜, ì—´ì˜ ê°œìˆ˜ë¥¼ ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” $$W^{O}$$ í–‰ë ¬ì„ ê³±í•´ì¤Œ.
  - Multi-headed attentionì—ì„œëŠ” $$W^O$$ ê°€ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•´ì•¼ë˜ëŠ” íŒŒë¼ë¯¸í„°ì„.
- Thatâ€™s pretty much all there is to multi-headed self-attention. Itâ€™s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place
  - ![transformer_multi-headed_self-attention-recap.png](\assets\images\transformer_multi-headed_self-attention-recap.png){: .align-center}

### Self-Attention êµ¬í˜„
- <https://github.com/google-research/bert/blob/master/modeling.py>  
  - BERT ì—ì„œ Transformerì˜ attentionì„ ì‚¬ìš©í•˜ê³  ìˆìŒ
<details><summary>Bert attention function</summary>
<script src="https://gist.github.com/marquis08/e551310b73a736f793d3fb834f047d3a.js"></script></details>





    


# Appendix
## MathJax
- bigger sum: $$\sum\limits_{\rm i=1}$$
```
$$\sum\limits_{\rm i=1}$$
```

## Reference
> Speech and Language Processing Chapter 9.4 self-attention network - Transformers: <https://web.stanford.edu/~jurafsky/slp3/ed3book_dec302020.pdf>  
> encoder decoder : <https://www.baeldung.com/cs/nlp-encoder-decoder-models>  
> A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification: <https://arxiv.org/abs/1510.03820>  
> The Illustrated Transformer: <https://jalammar.github.io/illustrated-transformer>  
> <https://ahnjg.tistory.com/57>