---
date: 2021-06-16 02:30
title: "ML basics - Probability Distributions"
categories: DevCourse2 ProbabilityDistributions MathJax
tags: DevCourse2 ProbabilityDistributions MathJax
# ëª©ì°¨
toc: true  
toc_sticky: true 
toc_label : "Contents"
---

# ë°€ë„ì¶”ì •(Density Estimation)
$$N$$ê°œì˜ ê´€ì°°ë°ì´í„°(observations) $$\mathbf{x}_1,\ldots\mathbf{x}_N$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë¶„í¬í•¨ìˆ˜ $$p(\mathbf{x})$$ë¥¼ ì°¾ëŠ” ê²ƒ

- $$p(\mathbf{x})$$ë¥¼ íŒŒë¼ë¯¸í„°í™”ëœ ë¶„í¬ë¡œ ê°€ì •í•œë‹¤. íšŒê·€, ë¶„ë¥˜ë¬¸ì œì—ì„œëŠ” ì£¼ë¡œ $$p(t\vert \mathbf{x})$$, $$p(\mathcal{C}\vert \mathbf{x})$$ë¥¼ ì¶”ì •í•œë‹¤.

- ê·¸ ë‹¤ìŒ ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ”ë‹¤.
    ë¹ˆë„ì£¼ì˜ ë°©ë²•(Frequentist's way): ì–´ë–¤ ê¸°ì¤€(ì˜ˆë¥¼ ë“¤ì–´ likelihood)ì„ ìµœì í™”ì‹œí‚¤ëŠ” ê³¼ì •ì„ í†µí•´ íŒŒë¼ë¯¸í„° ê°’ì„ ì •í•œë‹¤. íŒŒë¼ë¯¸í„°ì˜ í•˜ë‚˜ì˜ ê°’ì„ êµ¬í•˜ê²Œ ëœë‹¤.
    ë² ì´ì§€ì–¸ ë°©ë²•(Bayesian way): ë¨¼ì € íŒŒë¼ë¯¸í„°ì˜ ì‚¬ì „í™•ë¥ (prior distribution)ì„ ê°€ì •í•˜ê³  Bayes' ruleì„ í†µí•´ íŒŒë¼ë¯¸í„°ì˜ ì‚¬í›„í™•ë¥ (posterior distribution)ì„ êµ¬í•œë‹¤.

- íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•˜ë‹¤ë©´(í•œ ê°œì˜ ê°’ì´ë“  ë¶„í¬ë“ ) ê·¸ê²ƒì„ ì‚¬ìš©í•´ "ì˜ˆì¸¡"í•  ìˆ˜ ìžˆë‹¤($$t$$ë‚˜ $$\mathcal{C}$$).

ì¼¤ë ˆì‚¬ì „ë¶„í¬(Conjugate Prior): ì‚¬í›„í™•ë¥ ì´ ì‚¬ì „í™•ë¥ ê³¼ ë™ì¼í•œ í•¨ìˆ˜í˜•íƒœë¥¼ ê°€ì§€ë„ë¡ í•´ì¤€ë‹¤.  

# ì´í•­ë³€ìˆ˜(Binary Variables): ë¹ˆë„ì£¼ì˜ ë°©ë²•

ì´í•­ í™•ë¥ ë³€ìˆ˜(binary random variable) $$x\in \{0, 1\}$$ (ì˜ˆë¥¼ ë“¤ì–´ ë™ì „ë˜ì§€ê¸°)ê°€ ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤ê³  í•˜ìž.  

$$p(x=1 \vert  \mu) = \mu, p(x=0 \vert  \mu) = 1 - \mu$$

$$p(x)$$ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬(Bernoulli distribution)ë¡œ í‘œí˜„ë  ìˆ˜ ìžˆë‹¤.  

$$\mathrm{Bern}(x \vert  \mu) = \mu^x (1-\mu)^{1-x}$$

ê¸°ëŒ“ê°’, ë¶„ì‚°  
- \\(\mathbb{E}[x] = \mu\\)
- \\(\mathrm{var}[x] = \mu(1-\mu)\\)

ìš°ë„í•¨ìˆ˜ (Likelihood Function)

$$x$$ê°’ì„ $$N$$ë²ˆ ê´€ì°°í•œ ê²°ê³¼ë¥¼ $$\mathcal{D} = \{x_1,\ldots,x_N\}$$ë¼ê³  í•˜ìž. ê° $$x$$ê°€ ë…ë¦½ì ìœ¼ë¡œ $$p(x\vert \mu)$$ì—ì„œ ë½‘í˜€ì§„ë‹¤ê³  ê°€ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìš°ë„í•¨ìˆ˜($$\mu$$ì˜ í•¨ìˆ˜ì¸)ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆë‹¤.  

$$p(\mathcal{D}\vert \mu) = \prod_{n=1}^N p(x_n\vert \mu) = \prod_{n=1}^N \mu^{x_n} (1-\mu)^{1-x_n}$$

ë¹ˆë„ì£¼ì˜ ë°©ë²•ì—ì„œëŠ” $$\mu$$ê°’ì„ ì´ ìš°ë„í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ê°’ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìžˆë‹¤. ë˜ëŠ” ì•„ëž˜ì™€ ê°™ì´ ë¡œê·¸ìš°ë„í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”ì‹œí‚¬ ìˆ˜ë„ ìžˆë‹¤.  

$$\ln p(\mathcal{D}\vert \mu) = \sum_{n=1}^N \ln p(x_n\vert \mu) = \sum_{n=1}^N \{x_n\ln \mu + (1-x_n)\ln(1-\mu)\}$$

> ë¡œê·¸ë¡œ ë°”ê¿”ì¤˜ì„œ ê³± $$\Pi$$ì˜ í˜•íƒœì—ì„œ í•© $$\sum$$ì˜ í˜•íƒœë¡œ ë³€í™˜ì‹œí‚¨ë‹¤ëŠ”ë° ë¡œê·¸í–ˆì„ë•Œ í•©ìœ¼ë¡œ ì™œ ë˜ëŠ”ì§€  

$$\mu$$ì˜ ìµœëŒ€ìš°ë„ ì¶”ì •ì¹˜(maximum likelihood estimate)ëŠ”  

$$\mu^{\mathrm{ML}} = \frac{m}{N} ~~\mathrm{with}~~ m = (\#\mathrm{observations~of}~ x=1)$$

![binary-mu](/assets/images/binary-mu.png){: .align-center .img-80}  

$$N$$ì´ ìž‘ì€ ê²½ìš°ì— ìœ„ MLEëŠ” ê³¼ì í•©(overfitting)ëœ ê²°ê³¼ë¥¼ ë‚³ì„ ìˆ˜ ìžˆë‹¤. $$N = m = 3 \to \mu^{\mathrm{ML}} = 1$$!

# ì´í•­ë³€ìˆ˜(Binary Variables): ë² ì´ì§€ì–¸ ë°©ë²•

## ì´í•­ë¶„í¬ (Binomial Distribution)

$$\mathcal{D} = \{x_1,\ldots,x_N\}$$ì¼ ë•Œ, ì´í•­ë³€ìˆ˜ $$x$$ê°€ 1ì¸ ê²½ìš°ë¥¼ $$m$$ë²ˆ ê´€ì°°í•  í™•ë¥   

$$\mathrm{Bin}(m\vert N,\mu) = {N \choose m}\mu^m(1-\mu)^{N-m}$$  

$${N \choose m} = \frac{N!}{(N-m)!m!}$$

$$N$$ = data size  
$$\mu$$ = parameter  

- \\(\mathbb{E}[m] = \sum_{m=0}^N m\mathrm{Bin}(m\vert N,\mu) = N\mu\\)
- \\(\mathrm{var}[m] = \sum_{m=0}^N (m-\mathbb{E}[m])^2\mathrm{Bin}(m\vert N,\mu) = N\mu(1-\mu)\\)

ë°ì´í„°ë¥¼ ë³´ëŠ” ê´€ì 

- ë² ë¥´ëˆ„ì´ ì‹œí–‰ì˜ ë°˜ë³µ: $$x_1,\ldots,x_N$$ ê°ê°ì´ í™•ë¥ ë³€ìˆ˜
- $$x$$ê°€ 1ì¸ ê²½ìš°ë¥¼ ëª‡ ë²ˆ ê´€ì°°í–ˆëŠ”ê°€?: í•˜ë‚˜ì˜ í™•ë¥ ë³€ìˆ˜ $$m$$: binomialì¼ ê²½ìš°

ë² ì´ì§€ì•ˆ ë°©ë²•ì„ ì“°ê¸° ìœ„í•´ì„œ ë°ì´í„°ì˜ ìš°ë„ë¥¼ êµ¬í•´ì•¼ í•˜ëŠ”ë° ì´í•­ë¶„í¬ë¥¼ ê°€ì •í•˜ë©´ ìš°ë„í•¨ìˆ˜ê°€ í•˜ë‚˜ì˜ ë³€ìˆ˜ $$m$$ìœ¼ë¡œ($$x_1,\ldots,x_N$$ ëŒ€ì‹ ) í‘œí˜„ê°€ëŠ¥í•˜ë¯€ë¡œ ê°„íŽ¸í•´ì§„ë‹¤.

## ë² íƒ€ë¶„í¬ (Beta Distribution)

ë² ì´ì§€ì–¸ ë°©ë²•ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë² íƒ€ë¶„í¬ë¥¼ ì¼¤ë ˆì‚¬ì „ë¶„í¬(conjugate prior)ë¡œ ì‚¬ìš©í•œë‹¤.  

$$\mathrm{Beta}(\mu\vert a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$$

ê°ë§ˆí•¨ìˆ˜ $$\Gamma(x)$$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.  

$$\Gamma(x) = \int_0^{\infty}u^{x-1}e^{-u}\mathrm{d}u$$:  
$$u^{x-1}e^{-u}\mathrm{d}u$$ ì„ uì— ëŒ€í•´ì„œ 0~intê¹Œì§€ ì ë¶„í•œë‹¤.  


ê°ë§ˆí•¨ìˆ˜ëŠ” ê³„ìŠ¹(factorial)ì„ ì‹¤ìˆ˜ë¡œ í™•ìž¥ì‹œí‚¨ë‹¤. $$\Gamma(n) = (n-1)!$$
### $$\Gamma(x) = (x-1)\Gamma(x-1)$$ìž„ì„ ì¦ëª…í•˜ê¸°

Using integration by parts $$\int_0^{\infty}a\mathrm{d}b = \left. ab\right\vert_0^{\infty} - \int_0^{\infty}b\mathrm{d}a$$  

$$\begin{align*} a &= u^{x-1} &\ \mathrm{d}b &= -e^{-u}\mathrm{d}u\\ b &= e^{-u} &\ \mathrm{d}a &= (x-1)u^{x-2}\mathrm{d}u\\ \Gamma(x) &= \left. u^{x-1}(-e^{-u})\right\vert_0^{\infty} + \int_0^{\infty} (x-1)u^{x-2}e^{-u}\mathrm{d}u\\ &= 0 + (x-1)\Gamma(x-1) \end{align*}$$

### ë² íƒ€ë¶„í¬ê°€ normalizedìž„ì„ ì¦ëª…í•˜ê¸° ($$\int_0^{1}\mathrm{Beta}(\mu\vert a,b)\mathrm{d}\mu = 1$$)

$$\int_0^1 \mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$ìž„ì„ ì¦ëª…í•˜ë©´ ëœë‹¤.  

$$\begin{align*} \Gamma(a)\Gamma(b) &= \int_0^{\infty} x^{a-1}e^{-x}\mathrm{d}x\int_0^{\infty} y^{b-1}e^{-y}\mathrm{d}y\\ &= \int_0^{\infty}\int_0^{\infty}e^{-x-y}x^{a-1}y^{b-1}\mathrm{d}y\mathrm{d}x\\ &= \int_0^{\infty}\int_0^{\infty}e^{-t}x^{a-1}(t-x)^{b-1}\mathrm{d}t\mathrm{d}x &\ \mathrm{by}~ t=y+x, \mathrm{d}t = \mathrm{d}y\\ &= \int_0^{\infty}\int_0^{\infty}e^{-t}x^{a-1}(t-x)^{b-1}\mathrm{d}x\mathrm{d}t\\ &= \int_0^{\infty}e^{-t}\int_0^{\infty}x^{a-1}(t-x)^{b-1}\mathrm{d}x\mathrm{d}t\\ &= \int_0^{\infty}e^{-t}\int_0^1(t\mu)^{a-1}(t-t\mu)^{b-1}t\mathrm{d}\mu\mathrm{d}t &\ \mathrm{by}~ x=t\mu, \mathrm{d}x = t\mathrm{d}\mu\\ &= \int_0^{\infty}e^{-t}t^{a-1}t^{b-1}t\left(\int_0^1 \mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu\right)\mathrm{d}t\\ &= \int_0^{\infty}e^{-t}t^{a+b-1}\mathrm{d}t\int_0^1\mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu\\ &= \Gamma(a+b)\int_0^1\mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu \end{align*}$$

ë”°ë¼ì„œ, $$\int_0^1 \mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$ì´ ì„±ë¦½í•œë‹¤.

### ê¸°ëŒ“ê°’, ë¶„ì‚°  

- \\(\mathbb{E}[\mu] = \frac{a}{a+b}\\)
- \\(\mathrm{var}[\mu] = \frac{ab}{(a+b)^2(a+b+1)}\\)

![beta-expectations](/assets/images/beta-expectations.png){: .align-center .img-80}  

### $$\mu$$ì˜ ì‚¬í›„í™•ë¥  (posterior)  

$$l = N-m$$ ì „ì²´ë°ì´í„° Nì—ì„œ mì„ ëº€ ê°’ìœ¼ë¡œ ì •ì˜í•œë‹¤.  
$$a, b$$ëŠ” í˜„ìž¬ëŠ” í™•ë¥ ë³€ìˆ˜ê°€ ì•„ë‹ˆë¼ parameterì´ê³  ë‚˜ë¨¸ì§€ $$\mu,l,m$$ì€ í™•ë¥ ë³€ìˆ˜ìž„.  

$$\begin{align*} p(\mu \vert  m, l, a, b) &= \frac{\textrm{Bin}(m\vert N,\mu)\textrm{Beta}(\mu\vert a,b)}{\int_0^1 \textrm{Bin}(m\vert N,\mu)\textrm{Beta}(\mu\vert a,b)\textrm{d}\mu}\\ &= \frac{\mu^{m+a-1}(1-\mu)^{l+b-1}}{\int_0^1 \mu^{m+b-1}(1-\mu)^{l+b-1}\textrm{d}\mu}\\ &= \frac{\mu^{m+a-1}(1-\mu)^{l+b-1}}{\Gamma(m+a)\Gamma(l+b)/\Gamma(m+a+l+b)}\\ &= \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1} \end{align*}$$

![mu-posterior](/assets/images/mu-posterior.png){: .align-center .img-60}  

> $$\mu$$ì˜ ì‚¬ì „í™•ë¥ ì´ $$a$$, $$b$$ë¼ëŠ” parameterë¥¼ ê°–ê³  ìžˆë‹¤ê³  í•˜ê³ ,    
> m: x=1ì¸ ê²½ìš°ì˜ ê´€ì°°íšŸìˆ˜  
> l: x=0ì¸ ê²½ìš°ì˜ ê´€ì°°íšŸìˆ˜  
> ë¼ê³  í–ˆì„ ë•Œ, ìœ„ ì‹ì—ì„œ $$\mu$$ì™€ ê´€ë ¨ëœ ë¶€ë¶„ì˜ ì˜ë¯¸ë¥¼ ë³´ë©´,  

> $$\mu^{m+a-1}(1-\mu)^{l+b-1}$$ ì—¬ê¸°ì—ì„œ,  

> $$a$$ë¥¼ $$m$$ë§Œí¼ ì¦ê°€ì‹œí‚¤ê³ , $$b$$ë¥¼ $$l$$ë§Œí¼ ì¦ê°€ì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ë‚˜íƒ€ë‚œë‹¤ê³  ë³¼ ìˆ˜ ìžˆë‹¤.  
> ë°ì´í„° $$m$$,$$l$$ì´ ì£¼ì–´ì§€ê¸° ì „ì— ê°€ì§€ê³  ìžˆë˜ ì •ë³´ëŠ” $$a$$, $$b$$ ì˜€ëŠ”ë°  
> $$m$$,$$l$$ì´ë¼ëŠ” ìƒˆë¡œìš´ ê´€ì°° ê²°ê³¼ë¥¼ ì–»ì€ í›„ì—ëŠ” $$a$$ê°€ $$m$$ë§Œí¼ ì¦ê°€í•˜ê³  $$b$$ê°€ lë§Œí¼ ì¦ê°€í•˜ëŠ” íš¨ê³¼ë¥¼ ê°€ì§€ê²Œ ëœ ê²ƒì´ë‹¤.  

### ì—°ì†ì ì¸ ì—…ë°ì´íŠ¸

![consecutive-update](/assets/images/consecutive-update.png){: .align-center .img-80}  

í•˜ë‚˜ì˜ ìƒ˜í”Œì´ ìžˆê³  ê·¸ ê°’ì´ 1ì¼ë•Œì˜ likelihood  

$$Bin(1\vert 1, \mu) = \begin{pmatrix}1 \cr 1 \end{pmatrix} \mu^1(1-\mu)^0 = \mu$$  

> likelihood í•¨ìˆ˜ëŠ” ê¸°ìš¸ê¸°ê°€ 1ì¸ ì§ì„ í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤.  
> ì‚¬í›„í™•ë¥ ì€ prior x likelihood ì¸ í˜•íƒœë¡œ ë‚˜ì˜¬ ê²ƒì´ë‹¤.  
> ë‘ í•¨ìˆ˜ë¥¼ ê³±í•˜ê³  normalizeí–ˆì„ ë•Œ ìœ„ì™€ ê°™ì€ posterior ëª¨ìŠµì„ ë³´ì¸ë‹¤.  


> ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œì—ëŠ” ì§€ê¸ˆì€ $$\mu$$ì— ëŒ€í•œ ì‚¬í›„í™•ë¥ ì´ì§€ë§Œ ë‹¤ìŒë²ˆì— ìƒˆë¡œìš´ ë°ì´í„°ì— ì ìš©ë  ë•Œì—ëŠ”  
> $$\mu$$ì— ëŒ€í•œ ì‚¬ì „í™•ë¥ ë¡œ ì‚¬ìš©ë˜ë©´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í†µí•´ì„œ $$\mu$$ì— ëŒ€í•œ ì •ë³´ê°€ ê³„ì† ì—…ë°ì´íŠ¸ ë˜ëŠ” ê²ƒì´ë‹¤.  
> ë°ì´í„°ê°€ ìƒˆë¡­ê²Œ ë“¤ì–´ì˜¬ë•Œë§ˆë‹¤ ì •ë³´ê°€ ì—…ë°ì´íŠ¸ ëœë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.   

### ì˜ˆì¸¡ë¶„í¬ (predictive distribution)  
parameterë¥¼ êµ¬í•œ ë‹¤ìŒ ì˜ˆì¸¡ ë¶„í¬ë¥¼ êµ¬í•œë‹¤.  

$$p(x=1 \vert  \mathcal{D}) = \int_0^1 p(x=1\vert \mu)p(\mu\vert \mathcal{D})\mathrm{d}\mu = \int_0^1 \mu p(\mu\vert \mathcal{D})\mathrm{d}\mu = \mathbb{E}[\mu\vert \mathcal{D}]$$  

$$p(x=1 \vert  \mathcal{D}) = \frac{m+a}{m+a+l+b}$$

![predictive-distribution-1](/assets/images/predictive-distribution-1.png){: .align-center}  
![predictive-distribution-2](/assets/images/predictive-distribution-2.png){: .align-center}  


# ë‹¤í•­ë³€ìˆ˜(Multinomial Variables): ë¹ˆë„ì£¼ì˜ ë°©ë²•

$$K$$ê°œì˜ ìƒíƒœë¥¼ ê°€ì§ˆ ìˆ˜ ìžˆëŠ” í™•ë¥ ë³€ìˆ˜ë¥¼ $$K$$ì°¨ì›ì˜ ë²¡í„° $$\mathbf{x}$$ (í•˜ë‚˜ì˜ ì›ì†Œë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆë‹¤. ì´ëŸ° $$\mathbf{x}$$ë¥¼ ìœ„í•´ì„œ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë°˜í™”ì‹œí‚¬ ìˆ˜ ìžˆë‹¤.  

$$p(\mathbf{x}\vert \pmb \mu) = \prod_{k=1}^K \mu_k^{x_k}$$

with $$\sum_k \mu_k = 1$$

## $$\mathbf{x}$$ì˜ ê¸°ëŒ“ê°’  

$$\mathbb{E}[\mathbf{x}\vert \pmb \mu] = \sum_{\mathbf{x}} p(\mathbf{x}\vert \pmb \mu) = (\mu_1,\ldots,\mu_M)^T = \pmb \mu$$

![multinomial-expectations](/assets/images/multinomial-expectations.png){: .align-center}  


## ìš°ë„í•¨ìˆ˜

$$\bf x$$ê°’ì„ $$N$$ë²ˆ ê´€ì°°í•œ ê²°ê³¼ $$\mathcal{D} = \{\bf x_1,\ldots,\bf x_N\}$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìš°ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.  

$$p(\mathcal{D}\vert \pmb \mu) = \prod_{n=1}^N\prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{(\sum_n x_{nk})} = \prod_{k=1}^K \mu_k^{m_k}$$  

$$m_k = \sum_n x_{nk}$$

![multinomial-likelihood](/assets/images/multinomial-likelihood.png){: .align-center}  
![multinomial-likelihood-2](/assets/images/multinomial-likelihood-2.png){: .align-center}  

## MLE
$$\mu$$ì˜ ìµœëŒ€ìš°ë„ ì¶”ì •ì¹˜(maximum likelihood estimate)ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„  $$\mu_k$$ì˜ í•©ì´ 1ì´ ëœë‹¤ëŠ” ì¡°ê±´í•˜ì—ì„œ $$\ln p(\mathcal{D}\vert \pmb \mu)$$ì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” $$\mu_k$$ë¥¼ êµ¬í•´ì•¼ í•œë‹¤. ë¼ê·¸ëž‘ì£¼ ìŠ¹ìˆ˜(Lagrange multiplier) $$\lambda$$ë¥¼ ì‚¬ìš©í•´ì„œ ë‹¤ìŒì„ ìµœëŒ€í™”ì‹œí‚¤ë©´ ëœë‹¤.  

$$\sum_{k=1}^K m_k \ln \mu_k + \lambda \left(\sum_{k=1}^K \mu_k -1\right)$$  

$$\mu_k^{ML} = \frac{m_k}{N}$$  

> Constraint Optimization ë¬¸ì œì´ê¸° ë•Œë¬¸ì— ë¼ê·¸ëž‘ì£¼ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.  
> Equality Conditionì´ê¸° ë•Œë¬¸ì— lambdaë¥¼ ë¶™ì—¬ì¤€ë‹¤(+ or â€“ ê´€ê³„ ì—†ë‹¤.)  
> $$\mu_ð‘˜$$ ê°€ ì–´ë–¨ë•Œ ìš°ë„í•¨ìˆ˜ê°€ ìµœëŒ€í™” ë˜ëŠ”ì§€ë¥¼ êµ¬í•œë‹¤.  

> ë¼ê·¸ëž‘ì£¼ë¥¼ í™œìš©í•œ ì‹ ì „ì²´ì— ëŒ€í•´ $$\mu_ð‘˜$$ ì— ê´€í•´ì„œ ë¯¸ë¶„ì„ í•œë‹¤.  
> ê·¸ ê²°ê³¼ë¥¼ 0ìœ¼ë¡œ ë†“ê³  $$\mu_ð‘˜$$ì— ëŒ€í•´ í’€ë©´ ëœë‹¤.  

![multinomial-likelihood-estimation](/assets/images/multinomial-likelihood-estimation.png){: .align-center}  


## ë¼ê·¸ëž‘ì£¼ê°€ ë­”ì§€ ì—¬ê¸°ì— ì ì„ ê²ƒ

# ë‹¤í•­ë³€ìˆ˜(Multinomial Variables): ë² ì´ì§€ì–¸ ë°©ë²•

## ë‹¤í•­ë¶„í¬ (Multinomial distribution)

íŒŒë¼ë¯¸í„° $$\pmb \mu$$ì™€ ì „ì²´ ê´€ì°°ê°œìˆ˜ $$N$$ì´ ì£¼ì–´ì¡Œì„ ë•Œ $$m_1,\ldots,m_K$$ì˜ ë¶„í¬ë¥¼ ë‹¤í•­ë¶„í¬(multinomial distribution)ì´ë¼ê³  í•˜ê³  ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§„ë‹¤.  

$$\mathrm{Mult}(m_1,\ldots,m_K\vert \pmb \mu,N) = {N \choose m_1m_2\ldots m_K} \prod_{k=1}^K \mu_k^{m_k}$$  

$${N \choose m_1m_2\ldots m_K} = \frac{N!}{m_1!m_2!\ldots m_K!}$$  

$$\sum_{k=1}^K m_k= N$$  

$${N \choose m_1m_2\ldots m_K}$$: Nê°œì˜ ë¬¼ì²´ê°€ ìžˆì„ ë•Œ, ê°ê°ì˜ ì‚¬ì´ì¦ˆê°€ $$m_1m_2\ldots m_K$$ ê°€ ë˜ëŠ” kê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìžˆëŠ” ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ì˜ë¯¸  


## ë””ë¦¬í´ë ˆ ë¶„í¬(Dirichlet distribution): ë‹¤í•­ë¶„í¬ë¥¼ ìœ„í•œ ì¼¤ë ˆì‚¬ì „ë¶„í¬  
> ì´í•­ë³€ìˆ˜ë¥¼ ìœ„í•´ ì‚¬ìš©í–ˆë˜ ë² íƒ€ë¶„í¬ë¥¼ ì¼ë°˜í™”ì‹œí‚¨ í˜•íƒœ

$$\mathrm{Dir}(\pmb \mu\vert \mathbf{\alpha}) = \frac{\Gamma{\alpha_0}}{\Gamma(\alpha_1)\ldots\Gamma(\alpha_K)}\prod_{k=1}^K \mu_k^{\alpha_k-1}$$  

$$\alpha_0 = \sum_{k=1}^K \alpha_k$$  

## ë””ë¦¬í´ë ˆ ë¶„í¬ì˜ normalization ì¦ëª… ($$K=3$$)

ë‹¤ìŒ ê²°ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤.  
$$\begin{align*} \int_L^U(x-L)^{a-1}(U-x)^{b-1}\mathrm{d}x &= \int_0^1 (U-L)^{a-1}t^{a-1}(U-L)^{b-1}(1-t)^{b-1}(U-L)\mathrm{d}t &\ \mathrm{by}~ t=\frac{x-L}{U-L}\\ &= (U-L)^{a+b-1}\int_0^1 t^{a-1}(1-t)^{b-1}\mathrm{d}t\\ &= (U-L)^{a+b-1}\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \end{align*}$$  

$$\begin{align*} \int_0^{1-\mu_1}\mu_1^{\alpha_1-1}\mu_2^{\alpha_2-1}(1-\mu_1-\mu_2)^{\alpha_3-1}\mathrm{d}\mu_2 &= \mu_1^{\alpha_1-1}\int_0^{1-\mu_1}\mu_2^{\alpha_2-1}(1-\mu_1-\mu_2)^{\alpha_3-1}\mathrm{d}\mu_2 &\ \textrm{by}~ L=0, U=1-\mu_1\\ &= \mu_1^{\alpha_1-1}(1-\mu_1)^{\alpha_2+\alpha_3-1}\frac{\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_2+\alpha_3)} \end{align*}$$  

$$\begin{align*} \int_0^1\int_0^{1-\mu_1}\mu_1^{\alpha_1-1}\mu_2^{\alpha_2-1}(1-\mu_1-\mu_2)^{\alpha_3-1}\mathrm{d}\mu_2\mathrm{d}\mu_1 &= \frac{\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_2+\alpha_3)} \int_0^1 \mu_1^{\alpha_1-1}(1-\mu_1)^{\alpha_2+\alpha_3-1}\mathrm{d}\mu_1\\ &= \frac{\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_2+\alpha_3)} \frac{\Gamma(\alpha_1)\Gamma(\alpha_2+\alpha_3)}{\Gamma(\alpha_1+\alpha_2+\alpha_3)}\\ &= \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_1+\alpha_2+\alpha_3)} \end{align*}$$

## ì¼ë°˜ì ì¸ ê²½ìš°(k=M): ê·€ë‚©ë²•(induction)ìœ¼ë¡œ ì¦ëª…
base ê²½ìš°ê°€ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê³  N-1ì¸ ê²½ìš°ì— ì„±ë¦½ì„ í•œë‹¤ê³  ê°€ì •í•  ë•Œ,  
Nì¸ ê²½ìš°ì— ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©´ ëª¨ë“  Nì— ëŒ€í•´ì„œ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒ.  

base: M=2 ì¸ ë² íƒ€ë¶„í¬ì—ì„œ ì„±ë¦½  
M-1ê°œì˜ ë³€ìˆ˜ë“¤ì´ ìžˆì„ë•Œ ì„±ë¦½í•˜ë©´  
Mê°œì— ëŒ€í•´ì„œë„ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©´ ëœë‹¤.  

Mê°œì˜ ë³€ìˆ˜ë“¤ $$\mu_1 \sim \mu_ð‘€$$  
$$\mu_1+â€¦+\mu_{ð‘€âˆ’1} \le 1$$ ì´ê²ƒì„ ë§Œì¡±ì‹œí‚¤ë©´ ëœë‹¤.  
ì´ ì‹ì„ ë§Œì¡±ì‹œí‚¤ëŠ” ì ë¶„ì‹ì€  

![Dirichlet-normalize-1](/assets/images/Dirichlet-normalize-1.png){: .align-center}  
![Dirichlet-normalize-2](/assets/images/Dirichlet-normalize-2.png){: .align-center}  


## $$\mu$$ì˜ ì‚¬í›„í™•ë¥  (posterior)  

$$\begin{align*} p(\pmb \mu\vert \mathcal{D},\mathbf{\alpha}) &= \mathrm{Dir}(\pmb \mu\vert \mathbf{\alpha}+\mathbf{m})\\ &= \frac{\Gamma(\alpha_0+N)}{\Gamma(\alpha_1+m_1)\ldots\Gamma(\alpha_K+m_K)}\prod_{k=1}^K \mu_k^{\alpha_k+m_k-1} \end{align*}$$  

$$\mathbf{m} = (m_1,\ldots,m_K)^T$$

$$\alpha_k$$ë¥¼ $$x_k=1$$ì— ëŒ€í•œ ì‚¬ì „ê´€ì°° ê°œìˆ˜ë¼ê³  ìƒê°í•  ìˆ˜ ìžˆë‹¤.

```python
In [ ]:

# for inline plots in jupyter
%matplotlib inline
# import matplotlib
import matplotlib.pyplot as plt
# for latex equations
from IPython.display import Math, Latex
# for displaying images
from IPython.core.display import Image

In [ ]:

# import seaborn
import seaborn as sns
# settings for seaborn plotting style
sns.set(color_codes=True)
# settings for seaborn plot sizes
sns.set(rc={'figure.figsize':(5,5)})
import numpy as np

Uniform Distribution
In [ ]:

# import uniform distribution
from scipy.stats import uniform

In [ ]:

# random numbers from uniform distribution
n = 10000
start = 10
width = 20
data_uniform = uniform.rvs(size=n, loc = start, scale=width)

In [ ]:

data_uniform

In [ ]:

ax = sns.distplot(data_uniform,
                  bins=100,
                  kde=True,
                  color='skyblue',
                  hist_kws={"linewidth": 15,'alpha':1})
ax.set(xlabel='Uniform Distribution ', ylabel='Frequency')

Bernoulli Distribution
In [ ]:

from scipy.stats import bernoulli
data_bern = bernoulli.rvs(size=10000,p=0.8)

In [ ]:

np.unique(data_bern, return_counts=True)

In [ ]:

ax= sns.distplot(data_bern,
                 kde=False,
                 color="skyblue",
                 hist_kws={"linewidth": 15,'alpha':1})
ax.set(xlabel='Bernoulli Distribution', ylabel='Frequency')

Beta Distribution
In [ ]:

from scipy.stats import beta
a, b = 0.1, 0.1
data_beta = beta.rvs(a, b, size=10000)

In [ ]:

data_beta

In [ ]:

ax= sns.distplot(data_beta,
                 kde=False,
                 color="skyblue",
                 hist_kws={"linewidth": 15,'alpha':1})
ax.set(xlabel='Beta Distribution', ylabel='Frequency')

Multinomial Distribution
In [ ]:

from scipy.stats import multinomial
data_multinomial = multinomial.rvs(n=1, p=[0.2, 0.1, 0.3, 0.4], size=10000)

In [ ]:

data_multinomial[:50]

In [ ]:

for i in range(4):
  print(np.unique(data_multinomial[:,i], return_counts=True))

```  

# Appendix
## MathJax
Matrix with parenthesis $$\begin{pmatrix}1 \cr 1 \end{pmatrix}$$:  
```
$$\begin{pmatrix}1 \cr 1 \end{pmatrix}$$
```
## References

> Pattern Recognition and Machine Learning: <https://tensorflowkorea.files.wordpress.com/2018/11/bishop-pattern-recognition-and-machine-learning-2006.pdf>  