---
date: 2021-06-29 01:27
title: "NN basics - ML & Math"
categories: DevCourse2 NN MathJax
tags: DevCourse2 NN MathJax
## ëª©ì°¨
toc: true  
toc_sticky: true 
toc_label : "Contents"
---

# To do
## í¸ë¯¸ë¶„ (partial derivative)
{% include video id="AXqhWeUEtQU" provider="youtube" %}  
{% include video id="ly4S0oi3Yz8" provider="youtube" %}  
{% include video id="GkB4vW16QHI" provider="youtube" %}  

## Jacobian Matrix
<https://angeloyeo.github.io/2020/07/24/Jacobian.html>  
<https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/jacobian-prerequisite-knowledge>  

{% include video id="bohL918kXQk" provider="youtube" %}  
{% include video id="VmfTXVG9S0U" provider="youtube" %}  

## Hessian Matrix
2ì°¨ í¸ë„ í•¨ìˆ˜  

# Linear Algebra
## Vector & Matrix
### Matrix Multiplication & Vector Tranformation(function or mapping)

$$A\boldsymbol{x} = \boldsymbol{b}$$  

$$\begin{bmatrix} 4 & -3 & 1 & 3 \\ 2 & 0 & 5 & 1 \end{bmatrix}\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \\ \end{bmatrix} = \begin{bmatrix} 5 \\ 8 \\ \end{bmatrix}$$  

$$\boldsymbol{x} \in \mathbb{R}^{4} \rightarrow \boldsymbol{b} \in \mathbb{R}^2$$  

> xë¼ëŠ” vectorì— Aë¼ëŠ” í–‰ë ¬ì„ ê³±í–ˆì„ ë•Œ ìƒˆë¡œìš´ ê³µê°„ì— bë¼ëŠ” vectorë¡œ íˆ¬ì˜ ë¨. (xëŠ” 4ì°¨ì› ì‹¤ìˆ˜ ê³µê°„ì—ì„œ 2ì°¨ì› ì‹¤ìˆ˜ ê³µê°„ìœ¼ë¡œ)  

![matrix-multiplication-transformation](/assets/images/matrix-multiplication-transformation.png){: .align-center .img-70}  

Examples:  
![matrix-multiplication-transformation-2](/assets/images/matrix-multiplication-transformation-2.png){: .align-center}  

> í–‰ë ¬ì˜ ê³±ì…‰ì€ ê·¸ ëŒ€ìƒì´ ë²¡í„°ë“  í–‰ë ¬ì´ë“  ê³µê°„ì˜ ì„ í˜•ì  ë³€í™˜.  
>  
> ê²°êµ­ ì‹ ê²½ë§ì„ í†µí•´ representation learningì´ ê°€ëŠ¥í•œ ê²ƒ.  

#### Representaion Learning
ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ì™€ Represention Learning

ê¸°ì¡´ëŒ€ë¡œë¼ë©´ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ì—†ëŠ” ë°ì´í„°ê°€ ì„ í˜• ë¶„ë¦¬ê°€ ê°€ëŠ¥í•˜ê²Œë” ë°ì´í„°ê°€ ë³€í˜•ëë‹¤ëŠ” ì–˜ê¸°ì…ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´ ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ representaionì´ ($$x_1,x_2$$) ì—ì„œ ($$z_1,z_2$$) ë¡œ ë°”ë€ ê²ƒ.  

![representation-learning-vectors](/assets/images/representation-learning-vectors.png){: .align-center .img-30}  

ì´ ê¸€ì—ì„œëŠ” ì„¤ëª…ì˜ í¸ì˜ë¥¼ ìœ„í•´ ë‹¨ìˆœ ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ë¥¼ ì˜ˆë¡œ ë“¤ì—ˆìœ¼ë‚˜, ê¹Šê³  ë°©ëŒ€í•œ ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ëŠ” í•™ìŠµë°ì´í„°ê°€ ê½¤ ë³µì¡í•œ representionì´ì–´ë„ ì´ë¥¼ ì„ í˜• ë¶„ë¦¬ê°€ ê°€ëŠ¥í•  ì •ë„ë¡œ ë‹¨ìˆœí™”í•˜ëŠ” ë° ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ë•Œë¬¸ì— ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ë¥¼ representation learnerë¼ê³  ë¶€ë¥´ëŠ” ì‚¬ëŒë“¤ë„ ìˆìŠµë‹ˆë‹¤.

***
representation learningì´ë€, ì–´ë–¤ taskë¥¼ ìˆ˜í–‰í•˜ê¸°ì— ì ì ˆí•˜ê²Œ ë°ì´í„°ì˜ representationì„ ë³€í˜•í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰ ì–´ë–¤ taskë¥¼ ë” ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í‘œí˜„ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. Raw dataì— ë§ì€ feature engineeringê³¼ì •ì„ ê±°ì¹˜ì§€ ì•Šê³  ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒìœ¼ë¡œ, ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ìš”ì†Œë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ì˜ ìµœì ì˜ representationì„ ê²°ì •í•´ì£¼ê³  ì´ ì ì¬ëœ representationì„ ì°¾ëŠ” ê²ƒì„ representation learning ë˜ëŠ” feature learningì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.  

## Size of Vector & Matrix (Distance)
> ìœ ì‚¬ë„ëŠ” ë‚´ì ì„ í†µí•´ ì–´ëŠ ì •ë„ êµ¬í• ìˆ˜ ìˆìŒ.  
> 
> ê±°ë¦¬(size)ëŠ” Norm ìœ¼ë¡œ êµ¬í•¨.  
>  
> A norm is a function from a real or complex vector space to the nonnegative real numbers that behaves in certain ways **like the distance from the origin**.  

### Vector P Norm
Normì€ ë²¡í„°ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•  

#### P Norm
$$\Vert \boldsymbol{x}\Vert = \left(\sum_{i=1,d} \vert x_{i}^{p}\vert  \right)^{\frac{1}{p}}$$  

#### Absolute-value Norm(1ì°¨ ë†ˆ)
$$\vert \boldsymbol{x}_{1}\vert_{1}$$  

#### Euclidean Norm(2ì°¨ ë†ˆ)
$$\vert \boldsymbol{x}_{1}\vert_{2}$$  

#### Max Norm
$$\Vert \boldsymbol{x}_{\infty}\Vert = max(\vert x_{1}\vert,\ldots,\vert x_{d}\vert)$$  

> ì˜ˆ, x = (3, -4, 1)ì¼ ë•Œ, 2ì°¨ ë†ˆì€ $$\Vert \boldsymbol{x}_{1}\Vert_{2} = (3^2 +(-4)^2 + 1^2)^{1/2} \approx 5.099$$  
> ì´ê±´ ì›ì ìœ¼ë¡œ ë¶€í„°ì˜ ê±°ë¦¬ë¥¼ ì˜ë¯¸í•¨.  
>  
> ë²¡í„°ê°„ì˜ ê±°ë¦¬ë„ ê°€ëŠ¥í•¨.  
> $$\Vert \boldsymbol{z}_{1}\Vert_{2} = ((3-\boldsymbol{a})^2 +(-4-\boldsymbol{b})^2 + (1-\boldsymbol{c})^2)$$  

![type-of-norms](/assets/images/type-of-norms.png){: .align-center}  

### Matrix Frobenious Norm
í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ì¸¡ì •  

$$\Vert \boldsymbol{A}\Vert_{F} = \left( \sum_{i=1,n}\sum_{j=1,m}a_{ij}^2 \right)^{\frac{1}{2}}$$  

> ì˜ˆ, $$\boldsymbol{A} = \begin{bmatrix} 2 & 1 \\ 6 & 4 \end{bmatrix}$$ì¼ë•Œ,  
>  
> $$\Vert \boldsymbol{A}\Vert_{F} = \sqrt{2^2 + 1^2 + 6^2 + 4^2} = 7.550$$  

### Normì˜ í™œìš©
- ê±°ë¦¬(í¬ê¸°)ì˜ ê²½ìš°
- Regularizationì˜ ê²½ìš°
    - ![norm-regularization](/assets/images/norm-regularization.png){: .align-center}  

    - optimal pointì—ì„œ ìƒê¸¸ìˆ˜ ìˆëŠ” overfittingì„ í•´ê²°í•˜ê¸° ìœ„í•´ optimal pointì— ë„ë‹¬í•˜ì§€ ëª»í•˜ë„ë¡ l2 normì˜ boundaryì•ˆì—ì„œ optimal pointì™€ì˜ ìµœì†Œê°’ì„ ê°€ì§€ë„ë¡ Gradientì— Normì„ ì¶”ê°€í•˜ëŠ” í˜•íƒœë¡œ ì‚¬ìš©í•¨.  

## í¼ì…‰íŠ¸ë¡  (Perceptron)
input $$\boldsymbol{x}$$ì™€ $$\boldsymbol{W}$$ë¥¼ ë‚´ì í•œ ê²ƒì´ ê²°ê³¼ê°’ì¸ë° ì´ ê²°ê³¼ê°’ì— activation functionì„ ì ìš©í•´ì„œ threshold ê¸°ì¤€ìœ¼ë¡œ + or - í˜•íƒœë¡œ ë„ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.  

ë‚´ì ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë‚´ì ì„ í†µí•´ ë‚˜ì˜¨ scalarê°’ì´ activation functionì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤.  

<!-- ê³µê°„ì ìœ¼ë¡œ ë³´ë©´,  
í•™ìŠµì„ í†µí•´ ì–»ì–´ì§„ ê¸°ì¤€ì  wë¼ëŠ” ë²¡í„°ì™€ inputì´ ë‚´ì í•˜ëŠ” ê²°ê³¼(ìœ ì‚¬ì„±)ì´ Wë¥¼ ê¸°ì¤€ìœ¼ë¡œ activation functionì— ì˜í•´ ì£¼ì–´ì§„ thresholdì— ì˜í•´ ì–´ë–»ê²Œ êµ¬ë¶„ë˜ì–´ì§€ëŠ” ê°€.  

í•™ìŠµì„ í†µí•´ ê¸°ì¤€ì  wë¼ëŠ” ë²¡í„°ì™€ ì´ ë²¡í„°ì™€ ìˆ˜ì§ì¸ Decision boundaryê°€ ì–»ì–´ì§€ëŠ” ë°,  
ì£¼ì–´ì§„ inputì´ ì´ wë²¡í„°ì™€ ë‚´ì ì„ í•˜ë©´,  
ê·¸ ê°’ì´ decision boundary ê¸°ì¤€ìœ¼ë¡œ ë©€ê±°ë‚˜ ê°€ê¹ê±°ë‚˜ì˜ ì—¬ë¶€(activatoin functionìœ¼ë¡œ ë¶€í„° ê²°ì •ë˜ëŠ” threshold)ì— ë”°ë¼ output(+,-)ì´ ê²°ì •ë¨.   -->

ê³µê°„ì ìœ¼ë¡œ ë³´ë©´,  
í•™ìŠµì„ í†µí•´ ì–»ì–´ì§„ $$\boldsymbol{W}$$ì™€ input ì´ ë‚´ì ì„ í•œ ê°’ì„ $$\boldsymbol{W}$$ì— ëŒ€í•´ $$\text{Proj}$$í–ˆì„ ë•Œì˜ ìœ„ì¹˜ê°€, decision boundary ìƒì—ì„œ ì–´ë””ì— ìœ„ì¹˜í•´ ìˆëŠëƒ.  
ì´ $$\text{Proj}$$ ê°’ì´ í¼ì…‰íŠ¸ë¡ ì˜ activation functionì´ ì •í•œ threshold(T) ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì„ ê²½ìš° 1, ì•„ë‹ˆë©´ -1ì„ ouputìœ¼ë¡œ ë‚˜ì™€ì„œ ë¶„í•  í•´ì£¼ëŠ” ê²ƒì„.  

$$\boldsymbol{W}$$ì— ìˆ˜ì§ì¸ Decision boundaryëŠ” 2ì°¨ì›ì¼ë•ŒëŠ” decision line, 3ì°¨ì›ì¼ë•ŒëŠ” decision plane, 4ì°¨ì› ì´ìƒì€ decision hyperplane.  

> ê²°ì • ê²½ê³„ (decision boundary)ëŠ” ì„ í˜•íŒë³„í•¨ìˆ˜ $$y(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+w_0$$ê°€ 0ì„ ë§Œì¡±ì‹œí‚¤ëŠ” $$\boldsymbol{x}$$ì˜ ì§‘í•©  
> ë§Œì•½ biasë¥¼ ì‚¬ìš©í•  ê²½ìš° bias($$w_0$$)ê°€ ê²°ì • ê²½ê³„ë¥¼ ê²°ì •í•˜ëŠ” ìš”ì†Œê°€ ë¨(ê¸°ìš¸ê¸°ëŠ” ê°™ì§€ë§Œ ìœ„ì¹˜ ë‹¤ë¥¸).  
> [ì—¬ê¸°](https://marquis08.github.io/devcourse2/classification/mathjax/Linear-Models-for-Classification/#%ED%8C%90%EB%B3%84%ED%95%A8%EC%88%98-discriminant-functions)ë¥¼ ì°¸ì¡°.  

## Linear Classifier (cs231n)
![linear-classifier-1](/assets/images/linear-classifier-1.png){: .align-center}  
![linear-classifier-2](/assets/images/linear-classifier-2.png){: .align-center}  

ì´ë¯¸ì§€ X(tensor)ë¥¼ flatten ì‹œì¼œì„œ `32 x 32 x 3`ì¸ ë°ì´í„°ë¥¼ `3072 x 1`ì˜ shapeì¸ vectorìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì…ë ¥ê°’ìœ¼ë¡œ ë„£ì–´ì¤„ ê²ƒì´ê³ ,  
WëŠ” `10 x 3072`ì˜ shapeìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì´ í–‰ë ¬ê³¼ input vectorì™€ì˜ ë‚´ì ìœ¼ë¡œ ë‚˜ì˜¨ ê°’ì´ `10 x 1`ì˜ shapeì„ ì·¨í•˜ê²Œ ë§Œë“¤ì–´ì¤Œ. (10ê°œì˜ í¼ì…‰íŠ¸ë¡ ì´ ìˆë‹¤ê³  ë³´ë©´ ë¨)  
10ê°œì˜ target shapeì„ ë§Œë“  ê²ƒì€ ì„ì˜ì„. 3ê°œì˜ target classë¥¼ ì›í•œë‹¤ë©´ `3 x 1`ì˜ shapeìœ¼ë¡œ ì ìš©í•˜ë©´ ë¨.  

ì•„ë˜ì˜ slideë¥¼ ë³´ë©´,  
cat, dog and shipì¸ 3ê°œì˜ í´ë˜ìŠ¤ë¡œ êµ¬ë¶„í•˜ëŠ” ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œ Wì˜ shapeì´ `3 x 4`ì„ì„ ì•Œ ìˆ˜ ìˆìŒ.  

![linear-classifier-3](/assets/images/linear-classifier-3.png){: .align-center}  

> í¼ì…‰íŠ¸ë¡  í•˜ë‚˜ê°€ ì„ í˜• ë¶„ë¥˜ê¸° 1ê°œë¼ê³  ìƒê°í•˜ë©´ ë¨.  

> cs231n:  
> The problem is that the linear classifier is **only learning one template for each class**.  
> So, if there's sort of variations in how that class might appear, it's trying to average out all those different variations, all those different appearances, and **use just one single template to recognize each of those categories**.  

![linear-classifier-4](/assets/images/linear-classifier-4.png){: .align-center}  

> Images as points in high dimensional space. The linear classifier is putting in these linear decision boundaries to try to draw linear separation between one category and the rest of the categories.  

## ì—­í–‰ë ¬ (Inverse Matrix)
vectorë¥¼ transform ì‹œí‚¨ ë‹¤ìŒ ë‹¤ì‹œ ì›ê³µê°„ìœ¼ë¡œ ë˜ëŒë¦¬ê¸° ìœ„í•´ ì‚¬ìš©ë¨.  

ê³µê°„ì˜ ë³€í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê¸°ê³„í•™ìŠµ ëª¨ë¸ì´ë‚˜ ê°€ì„¤(PCA ê°™ì€ ê²½ìš°)ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì— ì—­í–‰ë ¬ì´ ê°€ëŠ¥í•˜ë©´ í¸ë¦¬í•´ì§€ëŠ” ë¶€ë¶„ì´ ì¡´ì¬í•¨.  

ì„ í˜•ëŒ€ìˆ˜ì—ì„œ ì—­í–‰ë ¬ì„ í†µí•´ ë°©ì •ì‹ì„ ë³´ë‹¤ í¸ë¦¬í•˜ê²Œ í’€ê¸° ìœ„í•¨.  

- ê°€ì—­í–‰ë ¬(Invertible Matrix)ì˜ ì¡°ê±´ ì¤‘ ì¤‘ìš”í•œ ê²ƒ.
    - $$\boldsymbol{A}$$ì˜ ëª¨ë“  í–‰ê³¼ ì—´ì´ ì„ í˜•ë…ë¦½ì´ë‹¤.
    - $$\det(\boldsymbol{A}) \neq 0$$.
    - $$\boldsymbol{A}^{T}\boldsymbol{A}$$ëŠ” positive definite ëŒ€ì¹­í–‰ë ¬ì„.
    - $$\boldsymbol{A}$$ì˜ eigenvalueëŠ” ëª¨ë‘ 0ì´ ì•„ë‹ˆë‹¤.

### í–‰ë ¬ì‹ (Determinant)
ê¸°í•˜í•™ì  ì˜ë¯¸: í–‰ë ¬ì˜ ê³±ì— ì˜í•œ ê³µê°„ì˜ í™•ì¥ ë˜ëŠ” ì¶•ì†Œ í•´ì„  
- $$\det(\boldsymbol{A}) = 0$$: í•˜ë‚˜ì˜ ì°¨ì›ì„ ë”°ë¼ ì¶•ì†Œë˜ì–´ ë¶€í”¼ë¥¼ ìƒê²Œ ë¨
- $$\det(\boldsymbol{A}) = 1$$: ë¶€í”¼ ìœ ì§€í•œ ë³€í™˜/ë°©í–¥ ë³´ì¡´ ë¨
- $$\det(\boldsymbol{A}) = -1$$: ë¶€í”¼ ìœ ì§€í•œ ë³€í™˜/ë°©í–¥ ë³´ì¡´ ì•ˆë¨
- $$\det(\boldsymbol{A}) = 5$$: 5ë°° ë¶€í”¼ í™•ì¥ë˜ë©° ë°©í–¥ ë³´ì¡´  

> ì›ê³µê°„ì— ëŒ€í•œ ë³€í™˜ì˜ ë¶€í”¼ì˜ ë³€í™”ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì„.  

### ì •ë¶€í˜¸ í–‰ë ¬ (positive definite matrices)
í–‰ë ¬ì˜ ê³µê°„ì˜ ëª¨ìŠµì„ íŒë‹¨í•˜ê¸° ìœ„í•´?  
ì–‘ì˜ ì •ë¶€í˜¸ í–‰ë ¬: 0ì´ ì•„ë‹Œ ëª¨ë“  ë²¡í„° $$\boldsymbol{x}$$ì— ëŒ€í•´, $$\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x} > 0$$  
ì„±ì§ˆ
- ê³ ìœ ê°’ ëª¨ë‘ ì–‘ìˆ˜
- ì—­í–‰ë ¬ë„ ì •ë¶€í˜¸ í–‰ë ¬
- $$\det(\boldsymbol{A}) = 0$$.  

### ë¶„í•´ (Decomposition)
#### ê³ ìœ ê°’ ë¶„í•´ (Eigen-decomposition)
[ML-Basics-Linear-Algebra](https://marquis08.github.io/devcourse2/linearalgebra/mathjax/ML-basics-Linear-Algebra/#%EA%B3%A0%EC%9C%A0%EA%B0%92-eigenvalues-%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0-eigenvectors) ì°¸ì¡°.  
> ì •ë°©í–‰ë ¬ $$A\in \mathbb{R}^{n\times n}$$ ì´ ì£¼ì–´ì¡Œì„ ë•Œ, $$Ax = \lambda x, x\neq 0$$ì„ ë§Œì¡±í•˜ëŠ” $$\lambda \in \mathbb{C}$$ë¥¼ $$A$$ì˜ ê³ ìœ ê°’(eigenvalue) ê·¸ë¦¬ê³  $$x\in \mathbb{C}^n$$ì„ ì—°ê´€ëœ ê³ ìœ ë²¡í„°(eigenvector)ë¼ê³  ë¶€ë¥¸ë‹¤.  
>  
> Eigenvectors: ì„ í˜•ë³€í™˜(T)ì´ ì¼ì–´ë‚œ í›„ì—ë„ **ë°©í–¥**ì´ ë³€í•˜ì§€ ì•ŠëŠ” ì˜ë²¡í„°ê°€ ì•„ë‹Œ ë²¡í„°.  
>  
> Eigenvalues: Eigenvectorsì˜ ê¸¸ì´ê°€ ë³€í•˜ëŠ” **ë°°ìˆ˜(scale)**, reversedë‚˜ scaledê°€ ë  ìˆ˜ ìˆì§€ë§Œ ë°©í–¥ì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤.  
> They make for interesting basis vectors. Basis vectors whos transformation matrices are maybe computationally more simpler or ones that make for better coordinate systems.  
>  
> numpy.linalg ëª¨ë“ˆì˜ eig í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.  

![the-effect-of-eigenvectors-and-eigenvalues](/assets/images/the-effect-of-eigenvectors-and-eigenvalues.png){: .align-center}  

> Figure 2.3: An example of the effect of eigenvectors and eigenvalues. Here, we have a matrix A with two orthonormal eigenvectors, v(1) with eigenvalue Î»1 and v(2) with eigenvalue Î»2. (Left) We plot the set of all unit vectors u âˆˆ R2 as a unit circle. (Right) We plot the set of all points Au. By observing the way that A distorts the unit circle, we can see that it scales space in direction v(i) by Î»i. *Deep Learning. Ian Goodfellow, Yoshua Bengio, and Aaron Courville.*  

ê³ ìœ ê°’ ë¶„í•´ë¥¼ í†µí•´ì„œ í–‰ë ¬ì˜ ì—­í–‰ë ¬ë„ êµ¬í•  ìˆ˜ ìˆê³ , PCAì—ì„œë„ í™œìš©í•¨.  

ê³ ìœ ê°’ ë¶„í•´ëŠ” ì •ì‚¬ê° í–‰ë ¬ì—ë§Œ ì ìš©ë¨.  
í•˜ì§€ë§Œ, MLì—ì„œ í•­ìƒ ì •ì‚¬ê° í–‰ë ¬ë§Œ ì¡´ì¬í•œë‹¤ëŠ” ë³´ì¥ì´ ì—†ê¸° ë•Œë¬¸ì— SVDë¥¼ ì‚¬ìš©.  

#### íŠ¹ì‡ê°’ ë¶„í•´ (SVD: Singular Value Decomposition)
ì •ì‚¬ê° í–‰ë ¬ì´ ì•„ë‹Œ í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë¨  


#### íŠ¹ì´ê°’ë¶„í•´(SVD)ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸
í–‰ë ¬ì„ $$x' = Ax$$ì™€ ê°™ì´ ì¢Œí‘œê³µê°„ì—ì„œì˜ ì„ í˜•ë³€í™˜ìœ¼ë¡œ ë´¤ì„ ë•Œ ì§êµí–‰ë ¬(orthogonal matrix)ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸ëŠ” íšŒì „ë³€í™˜(rotation transformation) ë˜ëŠ” ë°˜ì „ëœ(reflected) íšŒì „ë³€í™˜, ëŒ€ê°í–‰ë ¬(diagonal maxtrix)ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸ëŠ” ê° ì¢Œí‘œì„±ë¶„ìœ¼ë¡œì˜ ìŠ¤ì¼€ì¼ë³€í™˜(scale transformation)ì´ë‹¤.  

í–‰ë ¬ $$R$$ì´ ì§êµí–‰ë ¬(orthogonal matrix)ì´ë¼ë©´ $$RR^T = E$$ì´ë‹¤. ë”°ë¼ì„œ $$\det(RR^T) = \det(R)\det(R^T) = \det(R^2) = 1$$ì´ë¯€ë¡œ $$\det(R)$$ëŠ” í•­ìƒ +1, ë˜ëŠ” -1ì´ë‹¤. ë§Œì¼ $$\det(R)=1$$ë¼ë©´ ì´ ì§êµí–‰ë ¬ì€ **íšŒì „ë³€í™˜**ì„ ë‚˜íƒ€ë‚´ê³  $$\det(R)=-1$$ë¼ë©´ **ë’¤ì§‘í˜€ì§„(reflected) íšŒì „ë³€í™˜**ì„ ë‚˜íƒ€ë‚¸ë‹¤.  

ğŸ‘‰ ë”°ë¼ì„œ ì‹ $$(1), A = U\Sigma V^T$$ì—ì„œ $$U, V$$ëŠ” ì§êµí–‰ë ¬, $$\Sigma$$ëŠ” ëŒ€ê°í–‰ë ¬ì´ë¯€ë¡œ $$Ax$$ëŠ” $$x$$ë¥¼ ë¨¼ì € $$V^T$$ì— ì˜í•´ íšŒì „ì‹œí‚¨ í›„ $$\Sigma$$ë¡œ ìŠ¤ì¼€ì¼ì„ ë³€í™”ì‹œí‚¤ê³  ë‹¤ì‹œ $$U$$ë¡œ íšŒì „ì‹œí‚¤ëŠ” ê²ƒì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.  

![svd](/assets/images/svd.png){: .align-center}  
> <ê·¸ë¦¼1> ì¶œì²˜: ìœ„í‚¤í”¼ë””ì•„  

ğŸ‘‰ ì¦‰, í–‰ë ¬ì˜ íŠ¹ì´ê°’(singular value)ì´ë€ ì´ í–‰ë ¬ë¡œ í‘œí˜„ë˜ëŠ” **ì„ í˜•ë³€í™˜ì˜ ìŠ¤ì¼€ì¼ ë³€í™˜**ì„ ë‚˜íƒ€ë‚´ëŠ” ê°’ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.  

ğŸ‘‰ ê³ ìœ ê°’ë¶„í•´(eigendecomposition)ì—ì„œ ë‚˜ì˜¤ëŠ” ê³ ìœ ê°’(eigenvalue)ê³¼ ë¹„êµí•´ ë³´ë©´ ê³ ìœ ê°’ì€ ë³€í™˜ì— ì˜í•´ ë¶ˆë³€ì¸ ë°©í–¥ë²¡í„°(-> ê³ ìœ ë²¡í„°)ì— ëŒ€í•œ ìŠ¤ì¼€ì¼ factorì´ê³ , íŠ¹ì´ê°’ì€ ë³€í™˜ ìì²´ì˜ ìŠ¤ì¼€ì¼ factorë¡œ ë³¼ ìˆ˜ ìˆë‹¤.  

ğŸ‘‰ ì´ ì£¼ì œì™€ ê´€ë ¨í•˜ì—¬ ì¡°ê¸ˆ ë” ìƒìƒì˜ ë‚˜ë˜ë¥¼ í´ ë³´ë©´, $$m \times n$$ í–‰ë ¬ $$A$$ëŠ” $$n$$ì°¨ì› ê³µê°„ì—ì„œ $$m$$ì°¨ì› ê³µê°„ìœ¼ë¡œì˜ ì„ í˜•ë³€í™˜ì´ë‹¤. $$n$$ì°¨ì› ê³µê°„ì— ìˆëŠ” ì›, êµ¬ ë“±ê³¼ ê°™ì´ ì›í˜•ìœ¼ë¡œ ëœ ë„í˜•ì„ $$A$$ì— ì˜í•´ ë³€í™˜ì‹œí‚¤ë©´ ë¨¼ì € $$V^T$$ì— ì˜í•´ì„œëŠ” íšŒì „ë§Œ ì¼ì–´ë‚˜ë¯€ë¡œ ë„í˜•ì˜ í˜•íƒœëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ°ë° $$\Sigma$$ì— ì˜í•´ì„œëŠ” íŠ¹ì´ê°’ì˜ í¬ê¸°ì— ë”°ë¼ì„œ ì›ì´ íƒ€ì›ì´ ë˜ê±°ë‚˜ êµ¬ê°€ ëŸ­ë¹„ê³µì´ ë˜ëŠ” ê²ƒê³¼ ê°™ì€ ì‹ì˜ í˜•íƒœë³€í™˜ì´ ì¼ì–´ë‚œë‹¤ ($$n$$ì´ 2ì°¨ì›ì¸ ì›ì˜ ê²½ìš° ì²«ë²ˆì§¸ íŠ¹ì´ê°’ $$\sigma_1$$ì€ ë³€í™˜ëœ íƒ€ì›ì˜ ì£¼ì¶•ì˜ ê¸¸ì´, ë‘ë²ˆì§¸ íŠ¹ì´ê°’ $$\sigma_2$$ëŠ” ë‹¨ì¶•ì˜ ê¸¸ì´ì— ëŒ€ì‘ëœë‹¤). ì´í›„ $$U$$ì— ì˜í•œ ë³€í™˜ë„ íšŒì „ë³€í™˜ì´ë¯€ë¡œ ë„í˜•ì˜ í˜•íƒœì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•œë‹¤. ë§Œì¼ $$m > n$$ì´ë¼ë©´  0ì„ ë§ë¶™ì—¬ì„œ ì°¨ì›ì„ í™•ì¥í•œ í›„ì— $$U$$ë¡œ íšŒì „ì„ ì‹œí‚¤ëŠ” ê²ƒì´ê³  $$m < n$$ì´ë¼ë©´ ì¼ë¶€ ì°¨ì›ì„ ì—†ì• ë²„ë¦¬ê³ (ì¼ì¢…ì˜ íˆ¬ì˜) íšŒì „ì„ ì‹œí‚¤ëŠ” ì…ˆì´ë‹¤. **ê²°êµ­ ì„ í˜•ë³€í™˜ $$A$$ì— ì˜í•œ ë„í˜•ì˜ ë³€í™˜ê²°ê³¼ëŠ” í˜•íƒœì ìœ¼ë¡œ ë³´ë©´ ì˜¤ë¡œì§€ Aì˜ íŠ¹ì´ê°’(singular value)ë“¤ì— ì˜í•´ì„œë§Œ ê²°ì •ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.**  

# Information Theory & Optimization (chapter 3 in Deep Learning book)
í™•ë¥  ë¶„í¬ê°„ì˜ ìœ ì‚¬ì„±ì„ ì •ëŸ‰í™”  

ì •ë³´ì´ë¡ ì˜ ê¸°ë³¸ì›ë¦¬ ğŸ‘‰ í™•ë¥ ì´ ì‘ì„ìˆ˜ë¡ ë§ì€ ì •ë³´  
unlikely eventì˜ ì •ë³´ëŸ‰ì´ ë§ìŒ.  

## ìê¸° ì •ë³´ (self information)
ì‚¬ê±´(ë©”ì‹œì§€, $$e_i$$)ì˜ ì •ë³´ëŸ‰ (ë¡œê·¸ ë°‘ì´ 2ì¸ ê²½ìš° bit, ìì—°ìƒìˆ˜ì¸ ê²½ìš° nat)  
$$h(e_i) = -log_{2}P(e_{i})$$ or $$h(e_i) = -log_{e}P(e_{i})$$ 

> ì˜ˆ, ë™ì „ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì˜ ì •ë³´ëŸ‰:  
> $$log_{2}(\frac{1}{2}) = 1$$  
> 1~6ì¸ ì£¼ì‚¬ìœ„ì—ì„œ 1ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì˜ ì •ë³´ëŸ‰:  
> $$-log_{2}(\frac{1}{6}) \approx 2.58$$  
>  
> í›„ìì˜ ì‚¬ê±´ì´ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ ì •ë³´ëŸ‰ì„ ê°–ëŠ”ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŒ.  

## ì—”íŠ¸ë¡œí”¼ (Entropy)
í™•ë¥  ë³€ìˆ˜ $$x$$ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì—”íŠ¸ë¡œí”¼  
ëª¨ë“  ì‚¬ê±´ ì •ë³´ëŸ‰ì˜ ê¸°ëŒ“ê°’ìœ¼ë¡œ í‘œí˜„  

> ì´ì‚°í™•ë¥ ë¶„í¬ $$H(x) = - \sum_{i=1, k}P(e_i)log_{2}P(e_i)$$ ë˜ëŠ” $$H(x) = - \sum_{i=1, k}P(e_i)log_{e}P(e_i)$$  
> ì—°ì†í™•ë¥ ë¶„í¬ $$H(x) = - \int_{\mathbb{R}}P(x)log_{2}P(x)$$ ë˜ëŠ” $$H(x) = - \int_{\mathbb{R}}P(x)log_{e}P(x)$$   
>  
> ì˜ˆ, ë™ì „ì˜ ì•ë’¤ì˜ ë°œìƒ í™•ë¥ ì´ ë™ì¼í•œ ê²½ìš°ì˜ ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ  
>  
> $$\begin{align}H(x) &= - \sum_{i=1, k}P(e_i)logP(e_i) \\ &= - (0.5\times log_{2}0.5 + 0.5\times log_{2}0.5) \\ &= -log_{2}0.5 \\ &= -(-1) \end{align}$$  

ë™ì „ì˜ ë°œìƒ í™•ë¥ ì— ë”°ë¥¸ ì—”íŠ¸ë¡œí”¼ ë³€í™” (binary entrophy)  
![entropy-plot](/assets/images/entropy-plot.png){: .align-center}  
- ê³µí‰í•œ ë™ì „ì¼ ê²½ìš° ê°€ì¥ í° ì—”íŠ¸ë¡œí”¼ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
- ë™ì „ ë˜ì§€ê¸° ê²°ê³¼ ì „ì†¡ì—ëŠ” ìµœëŒ€ 1ë¹„íŠ¸ê°€ í•„ìš”í•¨ì„ ì˜ë¯¸

> ëª¨ë“  ì‚¬ê±´ì´ ë™ì¼í•œ í™•ë¥ ì„ ê°€ì§ˆ ë•Œ, ì¦‰, ë¶ˆí™•ì‹¤ì„±ì´ ê°€ì¥ ë†’ì€ ê²½ìš°, ì—”íŠ¸ë¡œí”¼ê°€ ìµœëŒ€ ê°’ì„ ê°–ëŠ”ë‹¤.  
>  
> ì˜ˆ, ìœ·ê³¼ pair ì£¼ì‚¬ìœ„(1~6)ì˜ ì—”íŠ¸ë¡œí”¼ ê°’ì„ ë¹„êµ.  
>  
> ìœ·: $$H(x) = - (\frac{4}{16}log_{2}\frac{4}{16} + \frac{6}{16}log_{2}\frac{6}{16} + \frac{4}{16}log_{2}\frac{4}{16} + \frac{1}{16}log_{2}\frac{1}{16} + \frac{1}{16}log_{2}\frac{1}{16}) \approx 2.0306 \text{ë¹„íŠ¸}$$  
>  
> ì£¼ì‚¬ìœ„: $$H(x) = - (\frac{1}{6}log_{2}\frac{1}{6} + \frac{1}{6}log_{2}\frac{1}{6} + \frac{1}{6}log_{2}\frac{1}{6} + \frac{1}{6}log_{2}\frac{1}{6} + \frac{1}{6}log_{2}\frac{1}{6} + \frac{1}{6}log_{2}\frac{1}{6}) \approx 2.585 \text{ë¹„íŠ¸}$$  

## êµì°¨ ì—”íŠ¸ë¡œí”¼ (Cross Entropy)
ë‘ ê°œì˜ í™•ë¥  ë¶„í¬ê°€ ì–¼ë§ˆ ë§Œí¼ì˜ ì •ë³´ë¥¼ ê³µìœ í•˜ëŠ” ê°€.  

$$H(P,Q) = - \sum_{x}P(x)log_{2}Q(x) = - \sum_{i=1, k} P(e_{i})log_{2}Q(e_{i})$$  

> Pë¼ëŠ” í™•ë¥ ë¶„í¬ì— ëŒ€í•´ì„œ Qì˜ ë¶„í¬ì˜ cross entropy  

ë”¥ëŸ¬ë‹ì—ì„œ outputì€ í™•ë¥ ê°’ì„.  
ì†ì‹¤í•¨ìˆ˜ëŠ” ì •ë‹µ(label or target)ê³¼ ì˜ˆì¸¡ê°’(prediction)ì„ ë¹„êµí•˜ê¸° ë•Œë¬¸ì—  
ì´ë¥¼ í™•ë¥ ê°’ìœ¼ë¡œ ë¹„êµí•˜ëŠ” ê²ƒì„.  

label ê°™ì€ ê²½ìš°ë„ OHEë¡œ í•˜ì§€ë§Œ ì´ê²ƒë„ 1ë¡œ ë˜ì–´ìˆëŠ” í™•ë¥  ë¶„í¬ê³  outputë„ í™•ë¥  ë¶„í¬ì´ê¸° ë•Œë¬¸ì—  
ì´ ì²™ë„ë¡œ ë¹„êµê°€ëŠ¥í•œ ê²ƒì´ ë°”ë¡œ CEì„.  

> ìœ„ì˜ ì‹ì„ ì „ê°œí•˜ë©´,  
>  
> $$\begin{align}H(P,Q) &= - \sum_{x}P(x)log_{2}Q(x) \\ &= - \sum_{x}P(x)log_{2}P(x) + \sum_{x}P(x)log_{2}P(x) - \sum_{x}P(x)log_{2}Q(x) \\  &= H(P) + \sum_{x}P(x)log_{2}\frac{P(x)}{Q(x)} \\ \end{align}$$  
>  
> $$- \sum_{x}P(x)log_{2}P(x) + \sum_{x}P(x)log_{2}P(x)$$ ì´ ì‹ì„ ì¶”ê°€í•´ì„œ ë³€í˜•í•œ ê²ƒì„.  
> ì´ ì‹ì„ í•©ì¹˜ë©´   
>  
>  $$\begin{align}\sum_{x}P(x)log_{2}P(x) - \sum_{x}P(x)log_{2}Q(x) \\ \Rightarrow \sum_{x}P(x)log_{2}\frac{P(x)}{Q(x)}\end{align}$$  
>  
> ì´ë ‡ê²Œ ë˜ëŠ” ë°, ì´ ì‹ì„ **KL Divergence** ë¼ê³  í•¨.  
>  
> ì—¬ê¸°ì„œ $$P$$ë¥¼ ë°ì´í„°ì˜ ë¶„í¬ë¼ê³  í•˜ë©´, ì´ëŠ” í•™ìŠµê³¼ì •ì—ì„œ ë³€í™”í•˜ì§€ ì•ŠìŒ.  
> $$P$$ëŠ” ê³ ì •ì´ê¸° ë•Œë¬¸ì— $$Q$$ë¥¼ ì¡°ì •í•´ì„œ cross entropyê°’ì„ ìµœì†Œí™” ì‹œí‚¤ëŠ” ê²ƒì„.  
>  
> Cross Entropyë¥¼ ì†ì‹¤í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°,  
>  
> $$H(P,Q) = H(P) + \sum_{x}P(x)log_{2}\frac{P(x)}{Q(x)}$$  
>  
> ì´ ì‹ì—ì„œ, $$P$$ëŠ” ê³ ì •ì´ê¸° ë•Œë¬¸ì— **KLD**ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•¨.  
>  
> ì¦‰, ê°€ì§€ê³  ìˆëŠ” ë°ì´í„° ë¶„í¬ P(x)ì™€ ì¶”ì •í•œ ë°ì´í„° ë¶„í¬ Q(x)ê°„ì˜ ì°¨ì´ ìµœì†Œí™” í•˜ëŠ”ë° êµì°¨ ì—”íŠ¸ë¡œí”¼ë¥¼ ì‚¬ìš©í•¨.  

## KLD (Kullbackâ€“Leibler divergence)
- Pì™€ Q ì‚¬ì´ì˜ KLD
- ë‘ í™•ë¥ ë¶„í¬ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•  ë•Œ ì£¼ë¡œ ì‚¬ìš©.  

$$KL(P\Vert Q) = \sum_{x}P(x)log_{2}\frac{P(x)}{Q(x)}$$  

> Pì™€ Qì˜ cross entrophyëŠ” pì˜ ì—”íŠ¸ë¡œí”¼ + Pì™€ Qê°„ì˜ KL ë‹¤ì´ë²„ì „ìŠ¤ì„.  

## Logit
DLì˜ outputì€ probabilityê°€ ë˜ì–´ì•¼ í•˜ëŠ”ë°,  
ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•´ì„œ ë‚˜ì˜¨ ê°’ì˜ ë²”ìœ„ëŠ” $$\left[-\infty, \infty \right]$$ì´ê¸° ë•Œë¬¸ì—  

activation function(sigmoid)ì„ ì ìš©í•´ì„œ í™•ë¥ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ”ë°, ì´ëŸ° ê²½ìš° ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ (ëª¨ë“  í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ë”í–ˆì„ ë•Œ 1ì´ ì•„ë‹˜)  
multilabel classificationì¼ ê²½ìš°ì— ê°€ëŠ¥í•˜ì§€ë§Œ.  
multiclassì˜ ê²½ìš°ì—ëŠ” ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ ì›í•˜ëŠ” ê²ƒì„(ëª¨ë“  í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ë”í–ˆì„ ë•Œ 1)  
ì´ ì—­í• ì„ í•´ì£¼ëŠ” ê²ƒì´ softmax functionì„  

í™•ë¥ ì—ì„œì˜ logit functionì€ $$log_{e}(\frac{p}{1-p})$$ ì„.  
pê°€ 0%ì— ê°€ê¹Œìš¸ ë•Œ logitì€ $$-\infty$$ì´ê³ , 100% ì— ê°€ê¹Œìš¸ ë•Œ logitì€ $$\infty$$ ì„.  

![logit](/assets/images/logit.png){: .align-center}  

ì¦‰ ë¡œì§“ì„ ê°€ì§€ê³  ìˆë‹¤ë©´($$\left[-\infty, \infty \right]$$), ì´ ë²”ìœ„ë¥¼ $$\left[0, 1\right]$$ë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì„.  

í™•ë¥ ì´ ì»¤ì§€ë©´ ë¡œì§“ë„ ì»¤ì§€ê¸° ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ì—ì„œ í™•ë¥ ëŒ€ì‹  ë¡œì§“ì„ ìŠ¤ì½”ì–´ë¡œë„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì„.  


ouputìœ¼ë¡œ ë¡œì§“ì´ ë‚˜ì˜¤ê³  ì´ê±¸ sigmoidë¡œ ë°”ê¾¸ë©´ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì´ ë‚˜ì˜´(ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ì„œ ê° í´ë˜ìŠ¤ê°€ ì•„ë‹˜, í•©í•˜ë©´ 1ì„ ë„˜ìŒ)  
softmaxëŠ” ë¡œì§“ì„ ì‚¬ìš©í•´ì„œ ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ ì–»ê²Œ í•´ì¤Œ.  
$$e^{logit}$$ì„ ì‚¬ìš©í•¨.  
$$e^{logit}$$ ì—­ì‹œ í™•ë¥ ì´ ì»¤ì§€ë©´ ì»¤ì§„ë‹¤ëŠ” ì„±ì§ˆì´ ìˆìŒ.  
$$e^{logit}$$ì„ ì‚¬ìš©í•˜ë©´ ë†’ì€ ì ìˆ˜ëŠ” ì•„ì£¼ ë†’ê²Œ ë˜ê³  ë‚®ì€ ì ìˆ˜ëŠ” ì•„ì£¼ ë‚®ê²Œ ë¨.  

ì´ê²ƒì´ ë”¥ëŸ¬ë‹ì— ë„ì›€ì´ ë˜ëŠ” ì´ìœ ëŠ” labelì„ OHE í•˜ê¸° ë•Œë¬¸ì„.  
í™•ë¥ ê°’ì´ ë†’ì€ ê²ƒë“¤ì„ ì•„ì£¼ ë†’ê²Œ, ë‚®ì€ ê°’ë“¤ì„ ì•„ì£¼ ë‚®ê²Œ í•˜ë©´ OHEì™€ ë¹„ìŠ·í•´ì§€ê¸° ë•Œë¬¸.  
ë”°ë¼ì„œ CrossEntropy í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ê²ƒì„.  

> why e in log?  
> 0ë³´ë‹¤ í° ì•„ë¬´ ê°’ì´ë‚˜ ì¨ë„ ìƒê´€ì€ ì—†ì§€ë§Œ(ìˆ˜í•™ì ìœ¼ë¡œëŠ”),  
> ë”¥ëŸ¬ë‹ì˜ outputì„ ë¡œì§“ì´ë¼ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì—, ë¡œì§“ì€ logì— e baseë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŒ.  
> ë”°ë¼ì„œ, $$e^{logit} = e^{log_{e}(\frac{p}{1-p})} = \frac{p}{1-p}$$ ê³„ì‚°ì´ ìƒë‹¹íˆ ê°„í¸í•´ì§.  
> ë‹¤ë¥¸ ìˆ«ìë¥¼ ì‚¬ìš©í•´ë„ softmaxì— ë¹„ìŠ·í•œ ê²°ê³¼ëŠ” ë‚˜ì˜¤ì§€ë§Œ, ë”¥ëŸ¬ë‹ ì•„ì´ë””ì–´ì˜ ê·¼ê±°ê°€ ë¡œì§“ì´ê¸° ë•Œë¬¸ì— eë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„.  
> ì˜¤ì¼ëŸ¬ ë„˜ë²„ë¼ê³  í•¨.  

![e-logit](/assets/images/e-logit.png){: .align-center .img-60}  

![logit-softmax](/assets/images/logit-softmax.png){: .align-center}  

> Summary  
> - Softmax gives probability distribution over predicted output classes.  
> - The final layer in deep learning has logit values which are raw values for prediction by softmax.  
> - Logit is the input to softmax  

## í¸ë¯¸ë¶„ (partial derivative)
- ë³€ìˆ˜ê°€ ë³µìˆ˜ì¸ í•¨ìˆ˜ì˜ ë¯¸ë¶„
- ë¯¸ë¶„ ê°’ì´ ì´ë£¨ëŠ” ë²¡í„°ë¥¼ gradientë¼ê³  ë¶€ë¦„.  


## Jacobian Matrix
í–‰ë ¬ì„ ë¯¸ë¶„í•œ ê²ƒ.  
1ì°¨ í¸ë„ í•¨ìˆ˜  

ì‹ ê²½ë§ì—ì„œ ì—°ì‚°ì€ í–‰ë ¬ë¡œ ì´ë£¨ì–´ì§€ê³ , ë¯¸ë¶„ì´ í•„ìš”í•œë° jacobianì„ í†µí•´ ë¯¸ë¶„í•¨.  

> ì˜ˆ,   
>  
> $$\boldsymbol{f}: \mathbb{R}^2 \mapsto \mathbb{R}^3 \ \boldsymbol{f(x)} = (2x_1+x_2^2,\  -x_1^2+3x_2,\  4x_1x_2)^T$$  
>  
> $$\boldsymbol{J} = \begin{pmatrix} 2 & 2x_2 \\ -2x_1 & 3 \\ 4x_2 & 4x_1 \end{pmatrix}\ \ \ \boldsymbol{J}\vert_{2,1}^{T} = \begin{pmatrix} 2 & 2 \\ -4 & 3 \\ 4 & 8 \end{pmatrix}$$  


## Appendix
### Positive Definite Matrices
{% include video id="cfn2ZUuWPd0" provider="youtube" %}  
{% include video id="ojUQk_GNQbQ" provider="youtube" %}  

### Geometric meaning of Determinant
{% include video id="Ip3X9LOh2dk" provider="youtube" %}  

### Reference
> Manifold Learning: <https://deepinsight.tistory.com/124>  
> Representation Learning: <https://ratsgo.github.io/deep%20learning/2017/04/25/representationlearning/>  
> Representation Learning: <https://velog.io/@tobigs-gnn1213/7.-Graph-Representation-Learning>  
> cs231n slides: <http://cs231n.stanford.edu/slides/2021/>  
> SVD: <https://darkpgmr.tistory.com/106>  
> logit: <https://youtu.be/K7HTd_Zgr3w>  
> partial derivative: <https://youtu.be/ly4S0oi3Yz8>, <https://youtu.be/GkB4vW16QHI>, <https://youtu.be/AXqhWeUEtQU>  
> jacobian matrix: <https://angeloyeo.github.io/2020/07/24/Jacobian.html>  


