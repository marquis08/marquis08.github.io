---
date: 2021-08-02 14:28
title: "NLP - Transformer and BERT"
categories: DevCourse2 NLP DevCourse2_NLP
tags: DevCourse2 NLP DevCourse2_NLP
# ëª©ì°¨
toc: true  
toc_sticky: true 
toc_label : "Contents"
---

# ML ëª¨ë¸ ì´í•´ë¥¼ ìœ„í•œ íŒ
- ë³µì¡í•œ ML ëª¨ë¸ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¥¼ ë”°ë¼ê°ˆ ê²ƒ
  - ì´ ëª¨ë¸ì´ í’€ë ¤ê³  í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€
  - ì¶”ë¡  ë‹¨ê³„ë¥¼ ë¨¼ì € ì´í•´. "í•™ìŠµ"ëœ ëª¨ë¸ì´ ì–´ë–¤ "ì…ë ¥"ì„ ë°›ì•„ì–´ ì–´ë–¤ "ì¶œë ¥"ì„ ê³„ì‚°í•˜ëŠ”ì§€ íŒŒì•…í•  ê²ƒ
  - ë¬´ì—‡ì´ í•™ìŠµë˜ëŠ”ê°€(ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì—‡ì¸ê°€), ëª¨ë¸ íŒŒë¼ë¯¸í„°ì™€ ì…ë ¥ ê·¸ë¦¬ê³  ê·¸ ë‘˜ì˜ í•¨ìˆ˜ë¥¼ êµ¬ë¶„í•  ê²ƒ.
  - ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ê°€(ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ê°€), ì–´ë–¤ ì—ëŸ¬í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°€

# NLP ì™€ ë”¥ëª¨ë¸
- NLPë¥¼ ìœ„í•œ ë”¥ëª¨ë¸ë“¤
  - RNN(LSTM ë“±)
  - TextCNN
  - Transformer
  - BERT (Transformer encoder)
  - GPT (Transformer decoder)

## RNN
![encoder_decoder2.png](\assets\images\encoder_decoder2.png){: .align-center}
- ë¬¸ì œì 
  - ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë“¤ ê°„ì˜ ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ì–´ë ¤ì›€
  - ìˆœì°¨ì  ì†ì„±ìœ¼ë¡œ ì¸í•œ ëŠë¦° ì†ë„

## TextCNN
![textcnn-arch.png](\assets\images\textcnn-arch.png){: .align-center}
- ì†ë„ëŠ” ë¹ ë¦„(conv ê³„ì‚°ì˜ ë³‘ë ¬í™”)
- ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë“¤ê°„ì˜ ì˜ì¡´ì„± ì–´ë ¤ì›€
- ë‚˜ì˜ì§€ ì•Šì€ ì„±ëŠ¥

## Transformer ëª¨ë¸
- Attention Is All You Need
  - No RNN, No convolution
  - ì˜¤ì§ attentionìœ¼ë¡œë§Œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ì˜ í‘œí˜„í•  ìˆ˜ ìˆë‹¤
  - ë³‘ë ¬í™” ê°€ëŠ¥
  - BERT, GPT ë“±ì˜ ëª¨ë¸ì˜ ê¸°ë°˜
### ì´ ëª¨ë¸ì´ í’€ë ¤ê³  í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€
  - seq2seq
  - ìˆœì°¨ì ì…ë ¥ì— ëŒ€í•´ ìˆœì°¨ì  ì¶œë ¥ì„ ë°˜í™˜
  - ê¸°ê³„ë²ˆì—­
  - ì§ˆì˜ì‘ë‹µ

### ì¶”ë¡  ë‹¨ê³„ ì´í•´
![the_transformer_3.png](\assets\images\the_transformer_3.png){: .align-center}
![The_transformer_encoders_decoders.png](\assets\images\The_transformer_encoders_decoders.png){: .align-center}
- The encoding component is a stack of encoders (the paper stacks six of them on top of each other â€“ thereâ€™s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.
![The_transformer_encoder_decoder_stack.png](\assets\images\The_transformer_encoder_decoder_stack.png){: .align-center}
- The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:
![encoder_with_tensors.png](\assets\images\encoder_with_tensors.png){: .align-center}
- í•˜ë‚˜ì˜ ë‹¨ì–´ë“¤ì´ ê°ê° ë‹¨ì–´ ì„ë² ë”©ì„ ê°€ì§€ê³  ìˆìŒ($$x_1$$),
- ì´ ì„ë² ë”©ì´ self-attention ëª¨ë“ˆì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°
- self-attention ëª¨ë“ˆì„ í†µê³¼í•˜ë©´ì„œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ì‚¬ìš©í•´ì„œ ê·¸ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ëª…í™•í•˜ê²Œ í•¨.(self-attentionì€ í•´ë‹¹ ë‹¨ì–´ ì£¼ë³€ì˜ ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•´ ë¬¸ë§¥ì„ í™œìš©í•¨, ë¬¸ë§¥í™”ëœ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…)
- ìƒˆë¡œìš´ embeddingì´ self-attention ëª¨ë“ˆì„ í†µê³¼í•˜ë©´ì„œ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë¨(ì´ ê²°ê³¼ëŠ” ì£¼ë³€ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ë‹¤ ì‚¬ìš©í•œ ê²ƒ)
- ì´ ì¶œë ¥ ê°’(embedding)ì´ Feed Forwardë¥¼ ì§€ë‚˜ê°, ê°ê°ì˜ ë‹¨ì–´ë“¤ì´ ë³„ê°œë¡œ ì§€ë‚˜ê°(ë™ì¼í•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©, ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì˜ì¡´ì„±ì´ ì—†ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™” ê°€ëŠ¥, íˆìë§Œ self-attentionì€ ë‹¨ì–´ê°„ì˜ ê´€ê³„ë¥¼ ë´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”ëŠ” í˜ë“¬)
- self-attentionì„ í†µê³¼í•œ í›„ Feed Forwardë¥¼ ì§€ë‚˜ê°€ê²Œ í•˜ëŠ” ì´ìœ ëŠ” í‘œí˜„ë ¥ì„ ë†’ì´ê¸° ìœ„í•¨ì´ë¼ê³  í•¨.(êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ê²ƒ?)

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Self-Attention
![transformer_self-attention_visualization.png](\assets\images\transformer_self-attention_visualization.png){: .align-center}
- "The animal didn't cross the street because it was too tired"
- ì—¬ê¸°ì„œ "it"ì´ ê°€ë¦¬í‚¤ëŠ” ë‹¨ì–´ëŠ”?
- ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ë¬¸ë§¥ì— ì˜í•´ ê²°ì •ë¨. ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ë§¥ì— ì˜í•´ ëœ»ì´ ë‹¬ë¼ì§
- í˜„ì¬ ë‹¨ì–´ì˜ ì˜ë¯¸(ì„ë² ë”©ì„ í†µí•´ í‘œí˜„ë˜ëŠ”)ë¥¼ ì£¼ë³€ ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì˜ ì¡°í•©(weighted sum)ìœ¼ë¡œ í‘œí˜„
- As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
- ![transformer_self-attention-weighted-sum.png](\assets\images\transformer_self-attention-weighted-sum.png){: .align-center}
  - ì´ë ‡ê²Œ weighted sumì„ í™œìš©í•´ì„œ "it"ì˜ embeddingì„ ê³„ì‚°í•˜ê³  ë‚˜ë©´, ê´€ë ¨ìˆëŠ” ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© ì„±ë¶„ì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆê²Œ ë¨.
  - ì´ëŸ°ì‹ìœ¼ë¡œ í•˜ë©´ ë¬¸ë§¥ì ì˜ë¯¸ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ.
  - RNNê³¼ ë¹„êµí•˜ë©´, RNNì˜ ê²½ìš° ë¬¸ë§¥ì˜ ì •ë³´ê°€ hidden stateë¥¼ í†µí•´ ì „ë‹¬ë˜ëŠ”ë°, ì´ê²ƒì€ ì´ì „ ë‹¨ê³„ì˜ ì •ë³´ë§Œì„ ë‹´ê³  ìˆìŒ.(unidirectional, ë‹¨ë°©í–¥)
  - TransformerëŠ” bidirectional, ì¦‰ ë¬¸ë§¥ ì „ì²´ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í›¨ì”¬ ì˜ë¯¸ê°€ í’ë¶€í•´ì§

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Self-Attention - ë‹¨ì–´ ì„ë² ë”© ë²¡í„° ì¤‘ì‹¬ìœ¼ë¡œ
1. The first step in calculating self-attention is to create three vectors from each of the encoderâ€™s input vectors (in this case, the embedding of each word).
    ![transformer_self_attention_vectors.png](\assets\images\transformer_self_attention_vectors.png){: .align-center}
    - Multiplying $$x_1$$ by the $$WQ$$ weight matrix produces $$q_1$$, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.
    - $$q^T = x^{T}W^{Q}$$, $$k^{T} = x^{T}W^{k}$$, $$v^{T} = x^{T}W^{v}$$
    - ì—¬ê¸°ì—ì„œ ì…ë ¥ì€ $$x^T$$ì´ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” í–‰ë ¬ë“¤($$W^{q}, W^{k}, W^{v}$$) ì„.
    - $$\underbrace{q^{T}}_{\text{í•¨ìˆ˜ê°’}} = \underbrace{x^{T}}_{\text{ì…ë ¥}}\underbrace{W^{q}}_{\text{ëª¨ë¸ íŒŒë¼ë¯¸í„°}}$$
    - ë³´í†µ xì˜ ì°¨ì›ì€ 512, qì˜ ì°¨ì›ì€ 64ë¥¼ ë§ì´ ì‚¬ìš©í•¨.
2. Self-attention score: The second step in calculating self-attention is to calculate a score.
    ![transformer_self_attention_score.png](\assets\images\transformer_self_attention_score.png){: .align-center}
    - "Thinking"ì˜ scoreë¥¼ êµ¬í•  ë•ŒëŠ”, thinkingì˜ ì¿¼ë¦¬ë²¡í„°($$q_1$$)ì„ ê°€ì§€ê³  ë‚˜ë¨¸ì§€ ëª¨ë“  ë‹¨ì–´ë“¤ì˜ í‚¤ë²¡í„°($$k_1, k_2$$)ì™€ì˜ dot productë¥¼ êµ¬í•¨(ìê¸°ìì‹  í¬í•¨)
3. Divide the scores by 8 (the square root of the dimension of the key vectors used in the paper â€“ 64. This leads to having more stable gradients.
4. Pass the result through a softmax operation. Softmax normalizes the scores so theyâ€™re all positive and add up to 1.
    - ![self-attention_softmax.png](\assets\images\self-attention_softmax.png){: .align-center}
    - ë…¼ë¬¸ ìƒì—ëŠ” embeddingì˜ ì‚¬ì´ì¦ˆ 64ì˜ rootê°’ì¸ 8ë¡œ ë‚˜ëˆ ì¤Œ(gradient ê³„ì‚°ì„ ì•ˆì •ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´ì„œ)

    - softmax ë¡œ ë³€í™˜(ê°ê°ì˜ ê°’ì´ í™•ë¥ ë¡œ ë¨)
    - ![attention-matrix-form.png](\assets\images\attention-matrix-form.png){: .align-center .img-40}
5. The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). 
6. The sixth step is to sum up the weighted value vectors.
    - ![self-attention-output.png](\assets\images\self-attention-output.png){: .align-center}
    - not embedding vector, its value vector.(weighted)
    - ìì‹ ì˜ ê°€ì¤‘ì¹˜ì¸ value vectoì™€ softmaxë¥¼ ê³±í•œë‹¤.
      - $$z_1 = \text{softmax value}\times v_1 + \text{softmax value}\times v_2 = 0.88 \times v_1 + 0.12 \times v_2$$
    - ![attention-matrix-score.png](\assets\images\attention-matrix-score.png){: .align-center .img-40}


#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Self-Attention - í–‰ë ¬ì—°ì‚°ìœ¼ë¡œ í‘œí˜„
- x: ì…ë ¥
- $$W^{q}, W^{k}, W^{v}$$: ëª¨ë¸ íŒŒë¼ë¯¸í„°
- $$Q, K, V$$: ì…ë ¥ê³¼ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ê´€í•œ í•¨ìˆ˜ì˜ ì¶œë ¥ê°’
- The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices weâ€™ve trained (WQ, WK, WV).
  - ![self-attention-matrix-calculation.png](\assets\images\self-attention-matrix-calculation.png){: .align-center}
- Finally, since weâ€™re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.
  - í–‰ë ¬ì˜ dimension ì •ë¦¬
    - s: sequence length, h:size_per_head(íŒŒë¼ë¯¸í„°ì˜ ì—´ì˜ ê°œìˆ˜, 64), d: input embedding size(xì˜ ì—´ì˜ ê°œìˆ˜, 512)
    - ![attention-matrix-all.png](\assets\images\attention-matrix-all.png){: .align-center}
  - ![self-attention-matrix-calculation-2.png](\assets\images\self-attention-matrix-calculation-2.png){: .align-center}
    - $$z = s \times h$$, h: size_per_head

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Multi-headed attention
- ë‹¤ì–‘í•œ attention matrixë“¤ì„ ë°˜ì˜í•˜ê¸° ìœ„í•œ ë°©ë²•. ë¸”ë¡œê·¸ ì˜¤ì—­ ì£¼ì˜ğŸš¨
- cnn ì—ì„œ í•„í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬ í˜•íƒœë¥¼ ë³´ê¸° ìœ„í–ˆë˜ ê²ƒ ì²˜ëŸ¼ í•™ìŠµëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´.
![transformer_attention_heads_qkv.png](\assets\images\transformer_attention_heads_qkv.png){: .align-center}
![transformer_attention_heads_z.png](\assets\images\transformer_attention_heads_z.png){: .align-center}
- This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices â€“ itâ€™s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix. How do we do that? We concat the matrices then multiple them by an additional weights matrix $$W^O$$.
  - ![transformer_attention_heads_weight_matrix_o.png](\assets\images\transformer_attention_heads_weight_matrix_o.png){: .align-center}
  - í–‰ì˜ ê°œìˆ˜ëŠ” ìœ ì§€í•˜ë˜, ì—´ì˜ ê°œìˆ˜ë¥¼ ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” $$W^{O}$$ í–‰ë ¬ì„ ê³±í•´ì¤Œ.
  - Multi-headed attentionì—ì„œëŠ” $$W^O$$ ê°€ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•´ì•¼ë˜ëŠ” íŒŒë¼ë¯¸í„°ì„.
- Thatâ€™s pretty much all there is to multi-headed self-attention. Itâ€™s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place
  - ![transformer_multi-headed_self-attention-recap.png](\assets\images\transformer_multi-headed_self-attention-recap.png){: .align-center}

### Self-Attention êµ¬í˜„
- <https://github.com/google-research/bert/blob/master/modeling.py>  
  - BERT ì—ì„œ Transformerì˜ attentionì„ ì‚¬ìš©í•˜ê³  ìˆìŒ
    - `attention_layer`
      - `from_tensor`: float Tensor of shape [batch_size, from_seq_length, from_width].
        - from_seq_length: ë¬¸ì¥ í˜¹ì€ ë¬¸ì„œì˜ ê¸¸ì´(ìƒ˜í”Œ ë‹¹ ëª‡ê°œì˜ ë‹¨ì–´ê°€ ë“¤ì–´ìˆëŠ”ì§€)
        - from_width: í•˜ë‚˜ì˜ ë‹¨ì–´ë‹¹ ëª‡ê°œì˜ ì°¨ì›ì„ ê°€ì§€ê³  ìˆëŠ” ì§€(ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ë² ë”©ì´ ëª‡ê°œì˜ ì°¨ì›ì„ ê°€ì§€ê³  ìˆëŠ”ì§€)
      - `to_tensor`: float Tensor of shape [batch_size, to_seq_length, to_width].
        - from_tensor ì™€ ë™ì¼í•œ êµ¬ì„±
      - `num_attention_heads`: int. Number of attention heads.
      - `size_per_head`: int. Size of each attention head. (ë³€í™˜ëœ ì„ë² ë”©ì˜ ì°¨ì›)
      - `Returns`: float Tensor of shape [batch_size, from_seq_length, **num_attention_heads * size_per_head**].
        - `num_attention_heads * size_per_head`: zë²¡í„°ë“¤ì„ concat í•œ ì‚¬ì´ì¦ˆ($$h \times N$$).
      - from_widthë¼ëŠ” ì‘ì€ ì°¨ì›ì´ ë“¤ì–´ê°”ë‹¤ê°€ í™•ì¥ë˜ì–´ì„œ return.
        - ì•ì—ì„œëŠ” Wë¥¼ ê³±í•´ì„œ ë‹¤ì‹œ ì›ë˜ì‚¬ì´ì¦ˆë¡œ ë³€í™˜í•œë‹¤ê³  í–ˆìŒ.(ê·¸ ë¶€ë¶„ì€ ì´ í•¨ìˆ˜ì—ëŠ” í¬í•¨ë˜ì–´ ìˆì§€ ì•Šê³ , ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” `transformer_model`ì´ë¼ëŠ” ë‹¤ë¥¸ í•¨ìˆ˜ ì•ˆì— ë“¤ì–´ìˆìŒ)
      - `query_layer`: Attention Matrix ê³„ì‚°ì‹œ ì²«ë²ˆì§¸ì˜€ë˜ í–‰ë ¬ Xì— W^{Q}ë¥¼ ê³±í•˜ëŠ” ë¶€ë¶„ ($$Q = XW^{Q}$$)
        ```py
        # Scalar dimensions referenced here:
        #   B = batch size (number of sequences)
        #   F = `from_tensor` sequence length
        #   T = `to_tensor` sequence length
        #   N = `num_attention_heads`
        #   H = `size_per_head`

        from_tensor_2d = reshape_to_matrix(from_tensor)
        to_tensor_2d = reshape_to_matrix(to_tensor)

        # `query_layer` = [B*F, N*H]
        query_layer = tf.layers.dense(
        from_tensor_2d,
        num_attention_heads * size_per_head,
        activation=query_act,
        name="query",
        kernel_initializer=create_initializer(initializer_range))
        ```
        - F, T ëŠ” encoder ë¶€ë¶„ì—ì„œ ê°™ì•„ì•¼í•¨( F=T(encoder) )
        - decoder ë¶€ë¶„ì—ì„œëŠ” ë‹¤ë¥¸ ê²½ìš°ê°€ ë‚˜ì˜´
        - $$Q = \overbrace{X}^{\text{from_tensor}}\overbrace{W^{Q}}^{\text{query_layer}}$$
        - `from_tensor`: $$[B, F, d]$$ (dëŠ” `from_width`)
          - í…ì„œ ì—°ì‚°ì€ $$[B, F, d]$$ ì¸ í…ì„œì˜ shapeì„ 2ì°¨ì›ìœ¼ë¡œ ë°”ê¾¼ë‹¤ &rarr; $$[B\times F, d]$$
            - `from_tensor_2d = reshape_to_matrix(from_tensor)` ì´ ë¼ì¸ì„ í†µí•´ ìˆ˜í–‰í•¨(custom í•¨ìˆ˜ì„)
              - <details><summary>reshape_to_matrix</summary><script src="https://gist.github.com/marquis08/8355516538ceae34bb91d68ca12e3cff.js"></script></details>
            - ì´ë ‡ê²Œ 2ì°¨ì› í–‰ë ¬ë¡œ ë³€í™˜í›„ $$W^{Q}$$ë¥¼ ê³±í•´ì£¼ëŠ” ì—°ì‚°ì„ í•´ì•¼í•˜ëŠ”ë°, ì´ê²ƒì€ ë”°ë¡œ ì •ì˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, tfì—ì„œ dense layerë¥¼ í†µí•´ì„œ í•¨.
              - ì´ dense layerì˜ input: `from_tensor_2d`, output: `num_attention_heads * size_per_head`(dense layerì—ì„œ ì¶œë ¥ì˜ ë…¸ë“œ ê°œìˆ˜)
              - ![dense-layer-wq.png](\assets\images\dense-layer-wq.png){: .align-center .img-40}
              - ì´ëŸ° í–‰ë ¬ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ê²Œ ë˜ëŠ” ê²ƒ.
          - $$Q$$ ê°€ `query_layer`ì— í•´ë‹¹. keyì™€ value ì—­ì‹œ ë§ˆì°¬ê°€ì§€ì„.
            - ì´ì œ Attention Matrix ê³„ì‚°ì´ ê°€ëŠ¥í•¨.
      - `attention_scores`: attention í–‰ë ¬ ê³„ì‚°
        ```py
        # Take the dot product between "query" and "key" to get the raw
        # attention scores.
        # `attention_scores` = [B, N, F, T]
        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
        attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))
        ```
        - `query_layer`, `key_layer`ìœ¼ë¡œ $$QK^{T}$$ë¥¼ ìˆ˜í–‰í•´ì•¼ í•¨. `transpose_b=True`ë¡œ Kë§Œ Transposeí•˜ê³  matmulë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì„.
        - ì´ë ‡ê²Œ í•œ í›„ softmaxë¥¼ ê³„ì‚°í•˜ë©´ ë˜ëŠ”ë°
        - ê·¸ ì „ì— í•´ì•¼í•  ê²ƒì´ ìˆìŒ
        - **Masking**
          - <details><summary>attention_mask</summary><script src="https://gist.github.com/marquis08/d58585ded6c3f2dde47b21ed66583ab9.js"></script></details>
          - í•„ìš”í•œ ì´ìœ : 
            - ì‹¤ì œ ë‹¨ì–´ì˜ ê°œìˆ˜ëŠ” ìƒ˜í”Œë§ˆë‹¤ ë‹¤ë¥¼ ê²ƒì„. 
            - ì…ë ¥ì—ì„œ ì‚¬ìš©í•œ `from_seq_length`ëŠ” ê³ ì •ëœ ê²ƒ. ê°ê°ì˜ ìƒ˜í”Œì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ì˜ ê°’ì„.
            - í•˜ì§€ë§Œ ëª¨ë“  ìƒ˜í”Œì´ ë™ì¼í•œ ê°œìˆ˜ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹˜.
            - ë¶€ì¡±í•œ ë¶€ë¶„ì€ paddingìœ¼ë¡œ ë‹¤ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ì±„ì›Œë„£ëŠ” ê²ƒì„.
            - attentionì„ ê³„ì‚°í•˜ë©´ì„œ ì´ë ‡ê²Œ ì±„ì›Œë„£ì€ paddingì— ëŒ€í•´ì„œ attentionì„ ê³„ì‚°í•˜ë©´ ì˜ë¯¸ê°€ ì—†ìŒ. (ì‹¤ì œ ë‹¨ì–´ì‚¬ì´ì˜ ê³„ì‚°ì´ í•„ìš”í•˜ë‹ˆê¹Œ)
          - ë°©ë²•:
            - attention_mask: ì–´ë– í•œ ë¶€ë¶„ì´ ì‹¤ì œ inputì¸ì§€
            - ì˜ˆ:
              - $$[[1,1,1],[1,1,0]]$$: ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ ê²½ìš° ì„¸ê°œì˜ ë‹¨ì–´ ëª¨ë‘ê°€ ì‹¤ì œ ìˆëŠ” ê²ƒì´ê³ , ë‘ ë²ˆì§¸ ìƒ˜í”Œì˜ ê²½ìš° ë§ˆì§€ë§‰ ë‹¨ì–´ëŠ” paddingì´ë¼ëŠ” ì˜ë¯¸
              - ëª¨ë“  ë‹¨ì–´ì˜ ì¡°í•©ì„ ê³„ì‚°í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— 3x3ì˜ í–‰ë ¬ì„ ë§Œë“ ë‹¤.
              - i ë²ˆì§¸ ë‹¨ì–´ì™€ j ë²ˆì§¸ ë‹¨ì–´ ì‚¬ì´ì˜ attentionì„ ê³„ì‚°í• ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆë‹¤. (1ì´ë©´ ê³„ì‚°í•˜ê³  0ì´ë©´ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”ë‹¤.)
              - ![attention-mask-ex.png](\assets\images\attention-mask-ex.png){: .align-center .img-40}
                - 2ë²ˆì§¸ ìƒ˜í”Œì˜ ê²½ìš° 1ë²ˆì§¸ ë‹¨ì–´ì™€ 3ë²ˆì§¸ ë‹¨ì–´ì˜ attentionì€ ê³„ì‚°í•˜ì§€ ì•ŠìŒ.
              - í•œë²ˆ ë” ê³¼ì •ì„ ê±°ì³ì„œ attention ê³„ì‚° ë¶€ë¶„ì„ 0ìœ¼ë¡œ masked ë¶€ë¶„ì„ -10000.0 ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
                - ![attention-mask-ex-2.png](\assets\images\attention-mask-ex-2.png){: .align-center .img-40}
              - ì´ë ‡ê²Œ ê³„ì‚°ëœ ë¶€ë¶„ì„ $$QK^{T}$$ì— ë”í•´ì£¼ë©´ ë¨.
                - **softmax** ê³„ì‚°ì„ í•˜ë©´ ìŒì˜ ë¬´í•œëŒ€ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì´ 0ì— ê°€ê¹Œìš´ ë¶€ë¶„ìœ¼ë¡œ ë³€í™˜ë¨.
            - masking matrixë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ë¶€ë¶„
              - <details><summary>create_attention_mask_from_input_mask</summary><script src="https://gist.github.com/marquis08/fe9b94ef6af25d11d320c7a8d904ed0a.js"></script></details>
      - `attention_probs = tf.nn.softmax(attention_scores)`ë¡œ ê³„ì‚°ì´ ë¨.
      - `context_layer = tf.matmul(attention_probs, value_layer)`ëŠ” $$ATT\times V$$ì˜ ì—°ì‚°ì„
      - masking ë¶€ë¶„ì€ decoder ë¶€ë¶„ì—ì„œë„ ì‚¬ìš©ë¨.
        - decoder ì˜ ê²½ìš° í˜„ì¬ ë‹¨ì–´ë³´ë‹¤ ì•ì˜ ë‹¨ì–´ë§Œ ì§‘ì¤‘í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì´ê±¸ í™œìš©í•´ì„œ ê°€ëŠ¥(ë’¤ì˜ ë‹¨ì–´ë¥¼ ìŒì˜ ë¬´í•œëŒ€ë¡œ)

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Positional encoding
- ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ê²ƒì¸ê°€
- ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë„£ì–´ì£¼ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŒ(íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ-ë¶„ë¥˜ì‘ì—…)
- ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” í¬ì§€ì…˜ ìì²´ë„ ìˆ«ìê°€ ì•„ë‹ˆë¼ ì„ë² ë”©ìœ¼ë¡œ í‘œí˜„í•˜ì.
- The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once theyâ€™re projected into Q/K/V vectors and during dot-product attention.
- ![transformer_positional_encoding_vectors.png](\assets\images\transformer_positional_encoding_vectors.png){: .align-center}
- ![transformer_positional_encoding_example.png](\assets\images\transformer_positional_encoding_example.png){: .align-center}
- í•™ìŠµí•  ë•Œ ë³´ì§€ ëª»í–ˆë˜ í¬ì§€ì…˜ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ
  - ì„ë² ë”©ì„ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•¨ìˆ˜ë¡œ í‘œí˜„(í•¨ìˆ˜ë¡œ ê³„ì‚°)

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Residual
- ![transformer_resideual_layer_norm.png](\assets\images\transformer_resideual_layer_norm.png){: .align-center}
  - dashed line ìœ¼ë¡œ í‘œì‹œëœ ë¶€ë¶„ì´ skip connection(residual connection)
    - Vanishing Gradientì„ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜

#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Encoder ì¢…í•©
- ![transformer_resideual_layer_norm_3.png](\assets\images\transformer_resideual_layer_norm_3.png){: .align-center}
- encoder ê°„ì˜ íŒŒë¼ë¯¸í„°ëŠ” ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤.


#### ì¶”ë¡  ë‹¨ê³„ ì´í•´ - Decoder
- ![transformer_decoding_1.gif](\assets\images\transformer_decoding_1.gif){: .align-center}
- encoder ì—ì„œ ë‚˜ì˜¨ tensorë¡œ Kí–‰ë ¬, Ví–‰ë ¬ì„ êµ¬í•¨(QëŠ” decoderì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê°ê°ì˜ ë²ˆì—­ë“¤ì´ queryë¡œ ì“°ì´ê¸° ë•Œë¬¸)
- encoder ì˜ ìµœì¢… ì¶œë ¥ì€ Kí–‰ë ¬, Ví–‰ë ¬
  - ì´ í–‰ë ¬ë“¤ì´ decoderì— ì „ë‹¬ë˜ê³  ì²« ë²ˆì§¸ ë‹¨ì–´ë¥¼ ì¶œë ¥í•¨
- ![transformer_decoding_2.gif](\assets\images\transformer_decoding_2.gif){: .align-center}
  - ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ ì¶œë ¥ì´ ë˜ë©´(decoder ì—ì„œ ë‚˜ì˜¨)
  - ê·¸ ë‹¤ìŒ inputìœ¼ë¡œ ì‚¬ìš©ë¨
- ![transformer_decoder_output_softmax.png](\assets\images\transformer_decoder_output_softmax.png){: .align-center}
  - The Final Linear and Softmax Layer
    - The decoder stack outputs a vector of floats. How do we turn that into a word? Thatâ€™s the job of the final Linear layer which is followed by a Softmax Layer.
    - The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.
    - Letâ€™s assume that our model knows 10,000 unique English words (our modelâ€™s â€œoutput vocabularyâ€) that itâ€™s learned from its training dataset. This would make the logits vector 10,000 cells wide â€“ each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.
    - The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.

### ëª¨ë¸í•™ìŠµ
- ì—ëŸ¬í•¨ìˆ˜ëŠ”? CrossEntropy
  - ![one-hot-vocabulary-example.png](\assets\images\one-hot-vocabulary-example.png){: .align-center .img-80}
  - ![transformer_logits_output_and_label.png](\assets\images\transformer_logits_output_and_label.png){: .align-center .img-80}
  - ![output_target_probability_distributions.png](\assets\images\output_target_probability_distributions.png){: .align-center .img-80}

# BERT
## ì´ ëª¨ë¸ì´ í’€ë ¤ê³  í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€
- Transfer Learningì„ í†µí•´ ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ì–‘ì§ˆì˜ ëª¨ë¸(ë¶„ë¥˜ê¸° ë“±)ì„ í•™ìŠµí•˜ëŠ” ê²ƒ
- ![bert-transfer-learning.png](\assets\images\bert-transfer-learning.png){: .align-center}

## ì¶”ë¡ ë‹¨ê³„ ì´í•´ - Fine-tuned ëª¨ë¸
- ![BERT-classification-spam.png](\assets\images\BERT-classification-spam.png){: .align-center}

## ì¶”ë¡ ë‹¨ê³„ ì´í•´ - Pre-trained ëª¨ë¸ - ì…ë ¥
- ![bert-input-output.png](\assets\images\bert-input-output.png){: .align-center}
- CLS í† í°ì„ ë¬¸ì¥ì˜ ì œì¼ ì²˜ìŒì— ë„£ì–´ì¤Œ.
- ![bert-encoders-input.png](\assets\images\bert-encoders-input.png){: .align-center}
- Transformer encoderì™€ ë™ì¼

## ì¶”ë¡ ë‹¨ê³„ ì´í•´ - Pre-trained ëª¨ë¸ - ì¶œë ¥
- ![bert-output-vector.png](\assets\images\bert-output-vector.png){: .align-center}
- í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•´ embedding ì¶œë ¥

## ì¶”ë¡ ë‹¨ê³„ ì´í•´ - Fine-tuned ëª¨ë¸ - ì¶œë ¥
- ![bert-classifier.png](\assets\images\bert-classifier.png){: .align-center}
- self-attentionì´ ë‹¤ ë°œìƒí•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ì²« ë²ˆì§¸ í† í°ì— ëŒ€í•´ì„œë„ ìê¸°ìì‹ ì— ëŒ€í•œ ë‚´ìš©ë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, inputì— ì‚¬ìš©ëœ ëª¨ë“  ë‹¨ì–´ë“¤ê°„ì˜ ê´€ê³„ë¡œ í‘œí˜„ë˜ì–´ìˆê¸° ë•Œë¬¸ì—, ëª¨ë“  ë‹¨ì–´ë“¤ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ì„ë² ë”©ì´ í•„ìš”ì—†ìŒ.
- classifierì—ëŠ” íŠ¹ìˆ˜ í† í°(CLS)ì˜ ì„ë² ë”©ì„ ì¸í’‹ìœ¼ë¡œ ì‚¬ìš©.
- íŠ¹ìˆ˜ í† í°(CLS)ì˜ ì„ë² ë”© ìœ„ì— ì›í•˜ëŠ” classifier ëª¨ë¸ì„ ìŒ“ìœ¼ë©´ ë¨. (ì¶”ê°€ ë°ì´í„°ë¡œ í•™ìŠµí•´ì•¼ í•˜ëŠ” ë¶€ë¶„)

## ëª¨ë¸ í•™ìŠµ - Pre-trained ëª¨ë¸
- BERT ëª¨ë¸ì´ Transformer ë¥¼ ì‚¬ìš©í•˜ê³  ìˆì§€ë§Œ, decoder ë¶€ë¶„ì´ ì—†ê¸° ë•Œë¬¸ì— encoderë§Œìœ¼ë¡œ ì–´ë–»ê²Œ í•™ìŠµí•´ì•¼ í• ê¹Œ
- Masked Language model
  - ì£¼ì–´ì§„ input ë‹¨ì–´ë“¤ ì¤‘ì— ëª‡ ê°œì˜ ë‹¨ì–´ë¥¼ ìˆ¨ê¹€
  - BERT ì˜ ì¶œë ¥ì„ í†µí•´ masked ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•¨
  - í•™ìŠµì„ í•˜ëŠ” ê²ƒì€ transformer ì•ˆì— ìˆëŠ” ëª¨ë¸ë“¤ì˜ parameter ë¿ë§Œ ì•„ë‹ˆë¼ inputìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” í† í°ë“¤ì˜ ì„ë² ë”©ë„ í•™ìŠµì´ ê°€ëŠ¥í•¨.
  - BERTëŠ” inputì´ ë‹¨ì–´ê°€ ì•„ë‹ˆë¼ sub word ë‹¨ìœ„ë¡œ í•™ìŠµí•¨ (wordpiece tokenizer ì‚¬ìš©)
  - ![BERT-language-modeling-masked-lm.png](\assets\images\BERT-language-modeling-masked-lm.png){: .align-center}

## ëª¨ë¸ í•™ìŠµ - Fine-tuned ëª¨ë¸
- Pre-trained ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ì´ˆë¡œ ìƒˆë¡œìš´ ì‘ì—…(ë¶„ë¥˜ ê°™ì€)ì„ ìœ„í•œ **ì ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©**í•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•¨.
- ![bert-tasks.png](\assets\images\bert-tasks.png){: .align-center}

## BERT - ì‘ìš©
- BERT ì˜ outputì„ ë‹¤ë¥¸ ëª¨ë¸ì˜ inputìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
- ì•ì—ì„œëŠ” CLS í† í°ì„ í†µí•´ì„œë§Œ ë¶„ë¥˜ taskì— ì‚¬ìš©
- contextualized word embeddings: pretrainedëœ BERT ëª¨ë¸ì˜ ì¶œë ¥ì„ ê·¸ëŒ€ë¡œ í•˜ë‚˜ì˜ í‘œí˜„(ë¬¸ì„œì „ì²´ì˜ í‘œí˜„)ìœ¼ë¡œ ì‚¬ìš©
- ![bert-contexualized-embeddings.png](\assets\images\bert-contexualized-embeddings.png){: .align-center}
- ì–´ë–¤ ì„ë² ë”©ì„ ì‚¬ìš©í•´ì•¼ ì œì¼ ì¢‹ì„ê¹Œ ë¼ëŠ” ì‹¤í—˜ì„ í–ˆìŒ
  - ![bert-feature-extraction-contextualized-embeddings.png](\assets\images\bert-feature-extraction-contextualized-embeddings.png){: .align-center}
- Data Augmentation
  - í´ë˜ìŠ¤ë¥¼ ë°”ê¾¸ì§€ ì•ŠëŠ” ë²”ìœ„ ì•ˆì—ì„œ ì…ë ¥ì„ ë³€í™˜. í•™ìŠµë°ì´í„°ë¥¼ í™•ì¥ì‹œí‚´. ë” ë‚˜ì€ ì¼ë°˜í™” ì„±ëŠ¥ ê¸°ëŒ€
  - ì´ë¯¸ì§€ì˜ ê²½ìš°: shift, filp, resize, rotate
  - í…ìŠ¤íŠ¸ëŠ”?
- BERT
  - ë¬¸ì„œ Dì™€ í´ë˜ìŠ¤ cê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Dì˜ ë‹¨ì–´ë“¤ì„ ëœë¤í•˜ê²Œ maskí•œ ë‹¤ìŒ BERT ë¥¼ ì‚¬ìš©í•´ì„œ ì˜ˆì¸¡í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ D'ë¡œ í•™ìŠµë°ì´í„°ì— ì¶”ê°€í•œë‹¤(í´ë˜ìŠ¤ cì™€ í•¨ê»˜)
  - GPT ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµë°ì´í„°ë¥¼ í™•ì¥í•  ìˆ˜ ìˆë‹¤.

# Appendix
# BERT Transformer
<details><summary>Bert attention function</summary>
<script src="https://gist.github.com/marquis08/e551310b73a736f793d3fb834f047d3a.js"></script></details>


## MathJax
- bigger sum: $$\sum\limits_{\rm i=1}$$
```
$$\sum\limits_{\rm i=1}$$
```

## Gist embedding
<script src="https://gist.github.com/marquis08/b025c83062e84d72ccd3c4276fa9bc5f.js"></script>  


## Reference
> Speech and Language Processing Chapter 9.4 self-attention network - Transformers: <https://web.stanford.edu/~jurafsky/slp3/ed3book_dec302020.pdf>  
> encoder decoder : <https://www.baeldung.com/cs/nlp-encoder-decoder-models>  
> A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification: <https://arxiv.org/abs/1510.03820>  
> The Illustrated Transformer: <https://jalammar.github.io/illustrated-transformer>  
> illustrated-bert:<https://jalammar.github.io/illustrated-bert/>
> transformer í•œê¸€ì •ë¦¬: <https://ahnjg.tistory.com/57>